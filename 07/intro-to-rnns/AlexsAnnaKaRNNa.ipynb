{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "vocab = set(text)\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([70, 55, 20, 67, 18, 36,  5, 52, 81, 10, 10, 10, 38, 20, 67, 67, 14,\n",
       "       52, 31, 20, 43,  1, 17,  1, 36,  6, 52, 20,  5, 36, 52, 20, 17, 17,\n",
       "       52, 20, 17,  1, 28, 36, 73, 52, 36, 21, 36,  5, 14, 52,  3, 24, 55,\n",
       "       20, 67, 67, 14, 52, 31, 20, 43,  1, 17, 14, 52,  1,  6, 52,  3, 24,\n",
       "       55, 20, 67, 67, 14, 52,  1, 24, 52,  1, 18,  6, 52,  7, 74, 24, 10,\n",
       "       74, 20, 14, 80, 10, 10,  8, 21, 36,  5, 14, 18, 55,  1, 24], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(chars)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    slice_size = batch_size * num_steps\n",
    "    n_batches = int(len(chars)/slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    y = chars[1:n_batches*slice_size+1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D mat\n",
    "    x = np.stack(np.split(x,batch_size))\n",
    "    y = np.stack(np.split(y,batch_size))\n",
    "    \n",
    "    # Now x and y are arrays with dimensions batch_size x n_batches*num_steps\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x, train_y = x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178650)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70, 55, 20, 67, 18, 36,  5, 52, 81, 10, 10, 10, 38, 20, 67, 67, 14,\n",
       "        52, 31, 20, 43,  1, 17,  1, 36,  6, 52, 20,  5, 36, 52, 20, 17, 17,\n",
       "        52, 20, 17,  1, 28, 36, 73, 52, 36, 21, 36,  5, 14, 52,  3, 24],\n",
       "       [52, 20, 43, 52, 24,  7, 18, 52, 34,  7,  1, 24, 34, 52, 18,  7, 52,\n",
       "         6, 18, 20, 14, 37, 32, 52, 20, 24,  6, 74, 36,  5, 36, 45, 52, 48,\n",
       "        24, 24, 20, 37, 52,  6, 43,  1, 17,  1, 24, 34, 37, 52, 54,  3],\n",
       "       [21,  1, 24, 80, 10, 10, 32, 15, 36,  6, 37, 52,  1, 18, 63,  6, 52,\n",
       "         6, 36, 18, 18, 17, 36, 45, 80, 52, 19, 55, 36, 52, 67,  5,  1, 44,\n",
       "        36, 52,  1,  6, 52, 43, 20, 34, 24,  1, 31,  1, 44, 36, 24, 18],\n",
       "       [24, 52, 45,  3,  5,  1, 24, 34, 52, 55,  1,  6, 52, 44,  7, 24, 21,\n",
       "        36,  5,  6, 20, 18,  1,  7, 24, 52, 74,  1, 18, 55, 52, 55,  1,  6,\n",
       "        10, 54,  5,  7, 18, 55, 36,  5, 52, 74, 20,  6, 52, 18, 55,  1],\n",
       "       [52,  1, 18, 52,  1,  6, 37, 52,  6,  1,  5, 61, 32, 52,  6, 20,  1,\n",
       "        45, 52, 18, 55, 36, 52,  7, 17, 45, 52, 43, 20, 24, 37, 52, 34, 36,\n",
       "        18, 18,  1, 24, 34, 52,  3, 67, 37, 52, 20, 24, 45, 10, 44,  5],\n",
       "       [52,  4, 18, 52, 74, 20,  6, 10,  7, 24, 17, 14, 52, 74, 55, 36, 24,\n",
       "        52, 18, 55, 36, 52,  6, 20, 43, 36, 52, 36, 21, 36, 24,  1, 24, 34,\n",
       "        52, 55, 36, 52, 44, 20, 43, 36, 52, 18,  7, 52, 18, 55, 36,  1],\n",
       "       [55, 36, 24, 52, 44,  7, 43, 36, 52, 31,  7,  5, 52, 43, 36, 37, 32,\n",
       "        52,  6, 55, 36, 52,  6, 20,  1, 45, 37, 52, 20, 24, 45, 52, 74, 36,\n",
       "        24, 18, 52, 54, 20, 44, 28, 52,  1, 24, 18,  7, 52, 18, 55, 36],\n",
       "       [73, 52, 54,  3, 18, 52, 24,  7, 74, 52,  6, 55, 36, 52, 74,  7,  3,\n",
       "        17, 45, 52,  5, 36, 20, 45,  1, 17, 14, 52, 55, 20, 21, 36, 52,  6,\n",
       "        20, 44,  5,  1, 31,  1, 44, 36, 45, 37, 52, 24,  7, 18, 52, 43],\n",
       "       [18, 52,  1,  6, 24, 63, 18, 80, 52, 19, 55, 36, 14, 63,  5, 36, 52,\n",
       "        67,  5,  7, 67,  5,  1, 36, 18,  7,  5,  6, 52,  7, 31, 52, 20, 52,\n",
       "         6,  7,  5, 18, 37, 10, 54,  3, 18, 52, 74, 36, 63,  5, 36, 52],\n",
       "       [52,  6, 20,  1, 45, 52, 18,  7, 52, 55, 36,  5,  6, 36, 17, 31, 37,\n",
       "        52, 20, 24, 45, 52, 54, 36, 34, 20, 24, 52, 20, 34, 20,  1, 24, 52,\n",
       "        31,  5,  7, 43, 52, 18, 55, 36, 52, 54, 36, 34,  1, 24, 24,  1]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_steps):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_steps: (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, batch_size=50, num_steps=50, lstm_size=128,\n",
    "             num_layers=2, learning_rate=0.001, \n",
    "              grad_clip=5, sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing\n",
    "    # in one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1,1\n",
    "        \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_steps], \n",
    "                            name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [batch_size, num_steps],\n",
    "                            name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for dropout layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    \n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add a dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop]* num_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is one step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, \n",
    "    # one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, axis=1)\n",
    "    output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "        \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will\n",
    "    # be a bunch of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilites for predicted characters\n",
    "    preds = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, \n",
    "                                                  labels=y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping\n",
    "    # to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state',\n",
    "                   'final_state', 'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_steps = 100\n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Iteration 1/3560 Training loss: 4.4167 4.2912 sec/batch\n",
      "Epoch 1/20  Iteration 2/3560 Training loss: 4.3680 2.9863 sec/batch\n",
      "Epoch 1/20  Iteration 3/3560 Training loss: 4.1695 2.9879 sec/batch\n",
      "Epoch 1/20  Iteration 4/3560 Training loss: 4.4170 2.9825 sec/batch\n",
      "Epoch 1/20  Iteration 5/3560 Training loss: 4.3546 2.9678 sec/batch\n",
      "Epoch 1/20  Iteration 6/3560 Training loss: 4.2901 3.2227 sec/batch\n",
      "Epoch 1/20  Iteration 7/3560 Training loss: 4.2255 3.2722 sec/batch\n",
      "Epoch 1/20  Iteration 8/3560 Training loss: 4.1528 3.1676 sec/batch\n",
      "Epoch 1/20  Iteration 9/3560 Training loss: 4.0801 3.8369 sec/batch\n",
      "Epoch 1/20  Iteration 10/3560 Training loss: 4.0169 7.5293 sec/batch\n",
      "Epoch 1/20  Iteration 11/3560 Training loss: 3.9603 5.5712 sec/batch\n",
      "Epoch 1/20  Iteration 12/3560 Training loss: 3.9118 4.8352 sec/batch\n",
      "Epoch 1/20  Iteration 13/3560 Training loss: 3.8703 4.2824 sec/batch\n",
      "Epoch 1/20  Iteration 14/3560 Training loss: 3.8357 3.8683 sec/batch\n",
      "Epoch 1/20  Iteration 15/3560 Training loss: 3.8027 3.6416 sec/batch\n",
      "Epoch 1/20  Iteration 16/3560 Training loss: 3.7730 3.5188 sec/batch\n",
      "Epoch 1/20  Iteration 17/3560 Training loss: 3.7447 3.4451 sec/batch\n",
      "Epoch 1/20  Iteration 18/3560 Training loss: 3.7216 3.3504 sec/batch\n",
      "Epoch 1/20  Iteration 19/3560 Training loss: 3.6999 3.4460 sec/batch\n",
      "Epoch 1/20  Iteration 20/3560 Training loss: 3.6781 3.4008 sec/batch\n",
      "Epoch 1/20  Iteration 21/3560 Training loss: 3.6588 3.4468 sec/batch\n",
      "Epoch 1/20  Iteration 22/3560 Training loss: 3.6409 3.5933 sec/batch\n",
      "Epoch 1/20  Iteration 23/3560 Training loss: 3.6239 3.6838 sec/batch\n",
      "Epoch 1/20  Iteration 24/3560 Training loss: 3.6087 3.5179 sec/batch\n",
      "Epoch 1/20  Iteration 25/3560 Training loss: 3.5939 3.4580 sec/batch\n",
      "Epoch 1/20  Iteration 26/3560 Training loss: 3.5802 3.5278 sec/batch\n",
      "Epoch 1/20  Iteration 27/3560 Training loss: 3.5679 3.4732 sec/batch\n",
      "Epoch 1/20  Iteration 28/3560 Training loss: 3.5556 3.5319 sec/batch\n",
      "Epoch 1/20  Iteration 29/3560 Training loss: 3.5441 3.4837 sec/batch\n",
      "Epoch 1/20  Iteration 30/3560 Training loss: 3.5334 3.5303 sec/batch\n",
      "Epoch 1/20  Iteration 31/3560 Training loss: 3.5242 3.5132 sec/batch\n",
      "Epoch 1/20  Iteration 32/3560 Training loss: 3.5145 3.4601 sec/batch\n",
      "Epoch 1/20  Iteration 33/3560 Training loss: 3.5049 3.5583 sec/batch\n",
      "Epoch 1/20  Iteration 34/3560 Training loss: 3.4964 4.0380 sec/batch\n",
      "Epoch 1/20  Iteration 35/3560 Training loss: 3.4877 3.7365 sec/batch\n",
      "Epoch 1/20  Iteration 36/3560 Training loss: 3.4800 3.5308 sec/batch\n",
      "Epoch 1/20  Iteration 37/3560 Training loss: 3.4719 3.7203 sec/batch\n",
      "Epoch 1/20  Iteration 38/3560 Training loss: 3.4643 3.5883 sec/batch\n",
      "Epoch 1/20  Iteration 39/3560 Training loss: 3.4569 3.4835 sec/batch\n",
      "Epoch 1/20  Iteration 40/3560 Training loss: 3.4497 3.4933 sec/batch\n",
      "Epoch 1/20  Iteration 41/3560 Training loss: 3.4429 3.7241 sec/batch\n",
      "Epoch 1/20  Iteration 42/3560 Training loss: 3.4366 3.9322 sec/batch\n",
      "Epoch 1/20  Iteration 43/3560 Training loss: 3.4301 3.8196 sec/batch\n",
      "Epoch 1/20  Iteration 44/3560 Training loss: 3.4241 3.9407 sec/batch\n",
      "Epoch 1/20  Iteration 45/3560 Training loss: 3.4183 3.7980 sec/batch\n",
      "Epoch 1/20  Iteration 46/3560 Training loss: 3.4130 3.9454 sec/batch\n",
      "Epoch 1/20  Iteration 47/3560 Training loss: 3.4079 3.7013 sec/batch\n",
      "Epoch 1/20  Iteration 48/3560 Training loss: 3.4031 3.5452 sec/batch\n",
      "Epoch 1/20  Iteration 49/3560 Training loss: 3.3984 3.7431 sec/batch\n",
      "Epoch 1/20  Iteration 50/3560 Training loss: 3.3938 3.5367 sec/batch\n",
      "Epoch 1/20  Iteration 51/3560 Training loss: 3.3892 3.7066 sec/batch\n",
      "Epoch 1/20  Iteration 52/3560 Training loss: 3.3846 4.2376 sec/batch\n",
      "Epoch 1/20  Iteration 53/3560 Training loss: 3.3805 4.0568 sec/batch\n",
      "Epoch 1/20  Iteration 54/3560 Training loss: 3.3761 3.8190 sec/batch\n",
      "Epoch 1/20  Iteration 55/3560 Training loss: 3.3722 3.6055 sec/batch\n",
      "Epoch 1/20  Iteration 56/3560 Training loss: 3.3680 3.6808 sec/batch\n",
      "Epoch 1/20  Iteration 57/3560 Training loss: 3.3641 4.1383 sec/batch\n",
      "Epoch 1/20  Iteration 58/3560 Training loss: 3.3605 4.1204 sec/batch\n",
      "Epoch 1/20  Iteration 59/3560 Training loss: 3.3567 3.8326 sec/batch\n",
      "Epoch 1/20  Iteration 60/3560 Training loss: 3.3533 3.6194 sec/batch\n",
      "Epoch 1/20  Iteration 61/3560 Training loss: 3.3499 3.6407 sec/batch\n",
      "Epoch 1/20  Iteration 62/3560 Training loss: 3.3469 4.0624 sec/batch\n",
      "Epoch 1/20  Iteration 63/3560 Training loss: 3.3441 4.1827 sec/batch\n",
      "Epoch 1/20  Iteration 64/3560 Training loss: 3.3407 3.8693 sec/batch\n",
      "Epoch 1/20  Iteration 65/3560 Training loss: 3.3375 3.8564 sec/batch\n",
      "Epoch 1/20  Iteration 66/3560 Training loss: 3.3347 3.9008 sec/batch\n",
      "Epoch 1/20  Iteration 67/3560 Training loss: 3.3318 3.8453 sec/batch\n",
      "Epoch 1/20  Iteration 68/3560 Training loss: 3.3285 3.9148 sec/batch\n",
      "Epoch 1/20  Iteration 69/3560 Training loss: 3.3256 3.8144 sec/batch\n",
      "Epoch 1/20  Iteration 70/3560 Training loss: 3.3230 4.0942 sec/batch\n",
      "Epoch 1/20  Iteration 71/3560 Training loss: 3.3202 4.6642 sec/batch\n",
      "Epoch 1/20  Iteration 72/3560 Training loss: 3.3179 3.7590 sec/batch\n",
      "Epoch 1/20  Iteration 73/3560 Training loss: 3.3154 3.6698 sec/batch\n",
      "Epoch 1/20  Iteration 74/3560 Training loss: 3.3129 4.0254 sec/batch\n",
      "Epoch 1/20  Iteration 75/3560 Training loss: 3.3106 4.4643 sec/batch\n",
      "Epoch 1/20  Iteration 76/3560 Training loss: 3.3084 4.0183 sec/batch\n",
      "Epoch 1/20  Iteration 77/3560 Training loss: 3.3061 3.7568 sec/batch\n",
      "Epoch 1/20  Iteration 78/3560 Training loss: 3.3039 3.6656 sec/batch\n",
      "Epoch 1/20  Iteration 79/3560 Training loss: 3.3016 4.0451 sec/batch\n",
      "Epoch 1/20  Iteration 80/3560 Training loss: 3.2991 4.4230 sec/batch\n",
      "Epoch 1/20  Iteration 81/3560 Training loss: 3.2969 4.0289 sec/batch\n",
      "Epoch 1/20  Iteration 82/3560 Training loss: 3.2949 3.9403 sec/batch\n",
      "Epoch 1/20  Iteration 83/3560 Training loss: 3.2928 4.0407 sec/batch\n",
      "Epoch 1/20  Iteration 84/3560 Training loss: 3.2907 3.7974 sec/batch\n",
      "Epoch 1/20  Iteration 85/3560 Training loss: 3.2884 3.8350 sec/batch\n",
      "Epoch 1/20  Iteration 86/3560 Training loss: 3.2862 4.3418 sec/batch\n",
      "Epoch 1/20  Iteration 87/3560 Training loss: 3.2840 4.5233 sec/batch\n",
      "Epoch 1/20  Iteration 88/3560 Training loss: 3.2819 3.9804 sec/batch\n",
      "Epoch 1/20  Iteration 89/3560 Training loss: 3.2802 3.9768 sec/batch\n",
      "Epoch 1/20  Iteration 90/3560 Training loss: 3.2785 4.0182 sec/batch\n",
      "Epoch 1/20  Iteration 91/3560 Training loss: 3.2768 3.9691 sec/batch\n",
      "Epoch 1/20  Iteration 92/3560 Training loss: 3.2750 4.1021 sec/batch\n",
      "Epoch 1/20  Iteration 93/3560 Training loss: 3.2732 3.9767 sec/batch\n",
      "Epoch 1/20  Iteration 94/3560 Training loss: 3.2715 4.0055 sec/batch\n",
      "Epoch 1/20  Iteration 95/3560 Training loss: 3.2697 3.7453 sec/batch\n",
      "Epoch 1/20  Iteration 96/3560 Training loss: 3.2679 3.6689 sec/batch\n",
      "Epoch 1/20  Iteration 97/3560 Training loss: 3.2663 4.0602 sec/batch\n",
      "Epoch 1/20  Iteration 98/3560 Training loss: 3.2645 4.4064 sec/batch\n",
      "Epoch 1/20  Iteration 99/3560 Training loss: 3.2628 3.9894 sec/batch\n",
      "Epoch 1/20  Iteration 100/3560 Training loss: 3.2611 3.7430 sec/batch\n",
      "Epoch 1/20  Iteration 101/3560 Training loss: 3.2594 3.6631 sec/batch\n",
      "Epoch 1/20  Iteration 102/3560 Training loss: 3.2578 4.0409 sec/batch\n",
      "Epoch 1/20  Iteration 103/3560 Training loss: 3.2561 4.4287 sec/batch\n",
      "Epoch 1/20  Iteration 104/3560 Training loss: 3.2544 4.3083 sec/batch\n",
      "Epoch 1/20  Iteration 105/3560 Training loss: 3.2528 4.3377 sec/batch\n",
      "Epoch 1/20  Iteration 106/3560 Training loss: 3.2512 3.9590 sec/batch\n",
      "Epoch 1/20  Iteration 107/3560 Training loss: 3.2493 3.9786 sec/batch\n",
      "Epoch 1/20  Iteration 108/3560 Training loss: 3.2475 3.9752 sec/batch\n",
      "Epoch 1/20  Iteration 109/3560 Training loss: 3.2458 3.9813 sec/batch\n",
      "Epoch 1/20  Iteration 110/3560 Training loss: 3.2438 3.9880 sec/batch\n",
      "Epoch 1/20  Iteration 111/3560 Training loss: 3.2420 3.9604 sec/batch\n",
      "Epoch 1/20  Iteration 112/3560 Training loss: 3.2401 4.6807 sec/batch\n",
      "Epoch 1/20  Iteration 113/3560 Training loss: 3.2382 4.6943 sec/batch\n",
      "Epoch 1/20  Iteration 114/3560 Training loss: 3.2362 4.2570 sec/batch\n",
      "Epoch 1/20  Iteration 115/3560 Training loss: 3.2343 3.8466 sec/batch\n",
      "Epoch 1/20  Iteration 116/3560 Training loss: 3.2324 4.0969 sec/batch\n",
      "Epoch 1/20  Iteration 117/3560 Training loss: 3.2304 3.8513 sec/batch\n",
      "Epoch 1/20  Iteration 118/3560 Training loss: 3.2287 4.1012 sec/batch\n",
      "Epoch 1/20  Iteration 119/3560 Training loss: 3.2269 3.8560 sec/batch\n",
      "Epoch 1/20  Iteration 120/3560 Training loss: 3.2249 4.0784 sec/batch\n",
      "Epoch 1/20  Iteration 121/3560 Training loss: 3.2231 3.8664 sec/batch\n",
      "Epoch 1/20  Iteration 122/3560 Training loss: 3.2211 4.1134 sec/batch\n",
      "Epoch 1/20  Iteration 123/3560 Training loss: 3.2191 3.8908 sec/batch\n",
      "Epoch 1/20  Iteration 124/3560 Training loss: 3.2172 4.0830 sec/batch\n",
      "Epoch 1/20  Iteration 125/3560 Training loss: 3.2151 3.8863 sec/batch\n",
      "Epoch 1/20  Iteration 126/3560 Training loss: 3.2127 4.0767 sec/batch\n",
      "Epoch 1/20  Iteration 127/3560 Training loss: 3.2105 3.8861 sec/batch\n",
      "Epoch 1/20  Iteration 128/3560 Training loss: 3.2084 4.0761 sec/batch\n",
      "Epoch 1/20  Iteration 129/3560 Training loss: 3.2061 3.8902 sec/batch\n",
      "Epoch 1/20  Iteration 130/3560 Training loss: 3.2037 4.0692 sec/batch\n",
      "Epoch 1/20  Iteration 131/3560 Training loss: 3.2014 4.9573 sec/batch\n",
      "Epoch 1/20  Iteration 132/3560 Training loss: 3.1988 4.4844 sec/batch\n",
      "Epoch 1/20  Iteration 133/3560 Training loss: 3.1963 4.0770 sec/batch\n",
      "Epoch 1/20  Iteration 134/3560 Training loss: 3.1937 3.8572 sec/batch\n",
      "Epoch 1/20  Iteration 135/3560 Training loss: 3.1908 4.0797 sec/batch\n",
      "Epoch 1/20  Iteration 136/3560 Training loss: 3.1881 3.8622 sec/batch\n",
      "Epoch 1/20  Iteration 137/3560 Training loss: 3.1869 4.2859 sec/batch\n",
      "Epoch 1/20  Iteration 138/3560 Training loss: 3.1850 4.9282 sec/batch\n",
      "Epoch 1/20  Iteration 139/3560 Training loss: 3.1833 4.3190 sec/batch\n",
      "Epoch 1/20  Iteration 140/3560 Training loss: 3.1812 4.3585 sec/batch\n",
      "Epoch 1/20  Iteration 141/3560 Training loss: 3.1791 4.2809 sec/batch\n",
      "Epoch 1/20  Iteration 142/3560 Training loss: 3.1770 3.9146 sec/batch\n",
      "Epoch 1/20  Iteration 143/3560 Training loss: 3.1748 4.0846 sec/batch\n",
      "Epoch 1/20  Iteration 144/3560 Training loss: 3.1725 3.9436 sec/batch\n",
      "Epoch 1/20  Iteration 145/3560 Training loss: 3.1702 4.0436 sec/batch\n",
      "Epoch 1/20  Iteration 146/3560 Training loss: 3.1680 3.9310 sec/batch\n",
      "Epoch 1/20  Iteration 147/3560 Training loss: 3.1657 4.0345 sec/batch\n",
      "Epoch 1/20  Iteration 148/3560 Training loss: 3.1635 3.9307 sec/batch\n",
      "Epoch 1/20  Iteration 149/3560 Training loss: 3.1609 4.0236 sec/batch\n",
      "Epoch 1/20  Iteration 150/3560 Training loss: 3.1584 3.9441 sec/batch\n",
      "Epoch 1/20  Iteration 151/3560 Training loss: 3.1561 4.0608 sec/batch\n",
      "Epoch 1/20  Iteration 152/3560 Training loss: 3.1538 3.9247 sec/batch\n",
      "Epoch 1/20  Iteration 153/3560 Training loss: 3.1512 4.0325 sec/batch\n",
      "Epoch 1/20  Iteration 154/3560 Training loss: 3.1487 4.8073 sec/batch\n",
      "Epoch 1/20  Iteration 155/3560 Training loss: 3.1460 4.6124 sec/batch\n",
      "Epoch 1/20  Iteration 156/3560 Training loss: 3.1433 4.1459 sec/batch\n",
      "Epoch 1/20  Iteration 157/3560 Training loss: 3.1405 4.5037 sec/batch\n",
      "Epoch 1/20  Iteration 158/3560 Training loss: 3.1376 4.1996 sec/batch\n",
      "Epoch 1/20  Iteration 159/3560 Training loss: 3.1347 3.8684 sec/batch\n",
      "Epoch 1/20  Iteration 160/3560 Training loss: 3.1319 4.3298 sec/batch\n",
      "Epoch 1/20  Iteration 161/3560 Training loss: 3.1291 4.8950 sec/batch\n",
      "Epoch 1/20  Iteration 162/3560 Training loss: 3.1261 4.3102 sec/batch\n",
      "Epoch 1/20  Iteration 163/3560 Training loss: 3.1230 4.3582 sec/batch\n",
      "Epoch 1/20  Iteration 164/3560 Training loss: 3.1201 4.2843 sec/batch\n",
      "Epoch 1/20  Iteration 165/3560 Training loss: 3.1172 3.9116 sec/batch\n",
      "Epoch 1/20  Iteration 166/3560 Training loss: 3.1143 4.0227 sec/batch\n",
      "Epoch 1/20  Iteration 167/3560 Training loss: 3.1113 4.8692 sec/batch\n",
      "Epoch 1/20  Iteration 168/3560 Training loss: 3.1083 4.5820 sec/batch\n",
      "Epoch 1/20  Iteration 169/3560 Training loss: 3.1054 4.1014 sec/batch\n",
      "Epoch 1/20  Iteration 170/3560 Training loss: 3.1024 3.8258 sec/batch\n",
      "Epoch 1/20  Iteration 171/3560 Training loss: 3.0995 4.1342 sec/batch\n",
      "Epoch 1/20  Iteration 172/3560 Training loss: 3.0967 3.9090 sec/batch\n",
      "Epoch 1/20  Iteration 173/3560 Training loss: 3.0940 4.1564 sec/batch\n",
      "Epoch 1/20  Iteration 174/3560 Training loss: 3.0912 5.0247 sec/batch\n",
      "Epoch 1/20  Iteration 175/3560 Training loss: 3.0883 4.3859 sec/batch\n",
      "Epoch 1/20  Iteration 176/3560 Training loss: 3.0855 4.3207 sec/batch\n",
      "Epoch 1/20  Iteration 177/3560 Training loss: 3.0826 4.3439 sec/batch\n",
      "Epoch 1/20  Iteration 178/3560 Training loss: 3.0796 3.9392 sec/batch\n",
      "Epoch 2/20  Iteration 179/3560 Training loss: 2.6084 4.0335 sec/batch\n",
      "Epoch 2/20  Iteration 180/3560 Training loss: 2.5671 4.7604 sec/batch\n",
      "Epoch 2/20  Iteration 181/3560 Training loss: 2.5540 4.6250 sec/batch\n",
      "Epoch 2/20  Iteration 182/3560 Training loss: 2.5491 4.1550 sec/batch\n",
      "Epoch 2/20  Iteration 183/3560 Training loss: 2.5453 3.8612 sec/batch\n",
      "Epoch 2/20  Iteration 184/3560 Training loss: 2.5433 4.1134 sec/batch\n",
      "Epoch 2/20  Iteration 185/3560 Training loss: 2.5412 3.8336 sec/batch\n",
      "Epoch 2/20  Iteration 186/3560 Training loss: 2.5394 4.1362 sec/batch\n",
      "Epoch 2/20  Iteration 187/3560 Training loss: 2.5386 4.9888 sec/batch\n",
      "Epoch 2/20  Iteration 188/3560 Training loss: 2.5364 4.4165 sec/batch\n",
      "Epoch 2/20  Iteration 189/3560 Training loss: 2.5336 4.0450 sec/batch\n",
      "Epoch 2/20  Iteration 190/3560 Training loss: 2.5323 3.9069 sec/batch\n",
      "Epoch 2/20  Iteration 191/3560 Training loss: 2.5303 4.4473 sec/batch\n",
      "Epoch 2/20  Iteration 192/3560 Training loss: 2.5307 4.8266 sec/batch\n",
      "Epoch 2/20  Iteration 193/3560 Training loss: 2.5291 4.2724 sec/batch\n",
      "Epoch 2/20  Iteration 194/3560 Training loss: 2.5272 3.9348 sec/batch\n",
      "Epoch 2/20  Iteration 195/3560 Training loss: 2.5256 4.0155 sec/batch\n",
      "Epoch 2/20  Iteration 196/3560 Training loss: 2.5262 3.9514 sec/batch\n",
      "Epoch 2/20  Iteration 197/3560 Training loss: 2.5249 4.0209 sec/batch\n",
      "Epoch 2/20  Iteration 198/3560 Training loss: 2.5218 4.8414 sec/batch\n",
      "Epoch 2/20  Iteration 199/3560 Training loss: 2.5195 4.5868 sec/batch\n",
      "Epoch 2/20  Iteration 200/3560 Training loss: 2.5193 4.2043 sec/batch\n",
      "Validation loss: 2.39037 Saving checkpoint!\n",
      "Epoch 2/20  Iteration 201/3560 Training loss: 2.5181 5.6367 sec/batch\n",
      "Epoch 2/20  Iteration 202/3560 Training loss: 2.5157 4.7537 sec/batch\n",
      "Epoch 2/20  Iteration 203/3560 Training loss: 2.5133 4.2298 sec/batch\n",
      "Epoch 2/20  Iteration 204/3560 Training loss: 2.5116 3.9073 sec/batch\n",
      "Epoch 2/20  Iteration 205/3560 Training loss: 2.5095 4.0457 sec/batch\n",
      "Epoch 2/20  Iteration 206/3560 Training loss: 2.5076 3.9015 sec/batch\n",
      "Epoch 2/20  Iteration 207/3560 Training loss: 2.5061 4.0424 sec/batch\n",
      "Epoch 2/20  Iteration 208/3560 Training loss: 2.5044 3.9041 sec/batch\n",
      "Epoch 2/20  Iteration 209/3560 Training loss: 2.5033 4.0316 sec/batch\n",
      "Epoch 2/20  Iteration 210/3560 Training loss: 2.5013 3.9165 sec/batch\n",
      "Epoch 2/20  Iteration 211/3560 Training loss: 2.4990 4.0650 sec/batch\n",
      "Epoch 2/20  Iteration 212/3560 Training loss: 2.4976 3.9151 sec/batch\n",
      "Epoch 2/20  Iteration 213/3560 Training loss: 2.4958 4.0392 sec/batch\n",
      "Epoch 2/20  Iteration 214/3560 Training loss: 2.4945 3.9168 sec/batch\n",
      "Epoch 2/20  Iteration 215/3560 Training loss: 2.4925 4.0187 sec/batch\n",
      "Epoch 2/20  Iteration 216/3560 Training loss: 2.4901 4.7706 sec/batch\n",
      "Epoch 2/20  Iteration 217/3560 Training loss: 2.4879 4.6197 sec/batch\n",
      "Epoch 2/20  Iteration 218/3560 Training loss: 2.4860 4.1285 sec/batch\n",
      "Epoch 2/20  Iteration 219/3560 Training loss: 2.4837 4.5143 sec/batch\n",
      "Epoch 2/20  Iteration 220/3560 Training loss: 2.4816 4.1040 sec/batch\n",
      "Epoch 2/20  Iteration 221/3560 Training loss: 2.4795 4.0500 sec/batch\n",
      "Epoch 2/20  Iteration 222/3560 Training loss: 2.4773 4.7426 sec/batch\n",
      "Epoch 2/20  Iteration 223/3560 Training loss: 2.4753 4.7126 sec/batch\n",
      "Epoch 2/20  Iteration 224/3560 Training loss: 2.4727 4.2965 sec/batch\n",
      "Epoch 2/20  Iteration 225/3560 Training loss: 2.4714 3.9381 sec/batch\n",
      "Epoch 2/20  Iteration 226/3560 Training loss: 2.4697 3.9961 sec/batch\n",
      "Epoch 2/20  Iteration 227/3560 Training loss: 2.4678 4.9602 sec/batch\n",
      "Epoch 2/20  Iteration 228/3560 Training loss: 2.4668 4.7938 sec/batch\n",
      "Epoch 2/20  Iteration 229/3560 Training loss: 2.4650 5.1354 sec/batch\n",
      "Epoch 2/20  Iteration 230/3560 Training loss: 2.4637 4.3721 sec/batch\n",
      "Epoch 2/20  Iteration 231/3560 Training loss: 2.4621 3.9955 sec/batch\n",
      "Epoch 2/20  Iteration 232/3560 Training loss: 2.4604 3.9329 sec/batch\n",
      "Epoch 2/20  Iteration 233/3560 Training loss: 2.4586 4.5360 sec/batch\n",
      "Epoch 2/20  Iteration 234/3560 Training loss: 2.4572 4.7583 sec/batch\n",
      "Epoch 2/20  Iteration 235/3560 Training loss: 2.4558 4.2246 sec/batch\n",
      "Epoch 2/20  Iteration 236/3560 Training loss: 2.4541 3.8965 sec/batch\n",
      "Epoch 2/20  Iteration 237/3560 Training loss: 2.4524 4.0526 sec/batch\n",
      "Epoch 2/20  Iteration 238/3560 Training loss: 2.4513 4.8749 sec/batch\n",
      "Epoch 2/20  Iteration 239/3560 Training loss: 2.4497 4.5401 sec/batch\n",
      "Epoch 2/20  Iteration 240/3560 Training loss: 2.4485 4.0842 sec/batch\n",
      "Epoch 2/20  Iteration 241/3560 Training loss: 2.4474 3.8288 sec/batch\n",
      "Epoch 2/20  Iteration 242/3560 Training loss: 2.4459 4.2050 sec/batch\n",
      "Epoch 2/20  Iteration 243/3560 Training loss: 2.4442 4.9751 sec/batch\n",
      "Epoch 2/20  Iteration 244/3560 Training loss: 2.4430 4.3552 sec/batch\n",
      "Epoch 2/20  Iteration 245/3560 Training loss: 2.4416 4.3203 sec/batch\n",
      "Epoch 2/20  Iteration 246/3560 Training loss: 2.4397 4.3098 sec/batch\n",
      "Epoch 2/20  Iteration 247/3560 Training loss: 2.4380 3.9432 sec/batch\n",
      "Epoch 2/20  Iteration 248/3560 Training loss: 2.4368 4.0635 sec/batch\n",
      "Epoch 2/20  Iteration 249/3560 Training loss: 2.4357 4.7535 sec/batch\n",
      "Epoch 2/20  Iteration 250/3560 Training loss: 2.4344 4.6433 sec/batch\n",
      "Epoch 2/20  Iteration 251/3560 Training loss: 2.4331 4.1192 sec/batch\n",
      "Epoch 2/20  Iteration 252/3560 Training loss: 2.4316 3.8233 sec/batch\n",
      "Epoch 2/20  Iteration 253/3560 Training loss: 2.4303 4.1267 sec/batch\n",
      "Epoch 2/20  Iteration 254/3560 Training loss: 2.4294 3.8383 sec/batch\n",
      "Epoch 2/20  Iteration 255/3560 Training loss: 2.4280 4.1227 sec/batch\n",
      "Epoch 2/20  Iteration 256/3560 Training loss: 2.4269 3.8655 sec/batch\n",
      "Epoch 2/20  Iteration 257/3560 Training loss: 2.4254 4.1013 sec/batch\n",
      "Epoch 2/20  Iteration 258/3560 Training loss: 2.4241 5.0236 sec/batch\n",
      "Epoch 2/20  Iteration 259/3560 Training loss: 2.4225 4.4172 sec/batch\n",
      "Epoch 2/20  Iteration 260/3560 Training loss: 2.4214 4.2581 sec/batch\n",
      "Epoch 2/20  Iteration 261/3560 Training loss: 2.4199 4.3860 sec/batch\n",
      "Epoch 2/20  Iteration 262/3560 Training loss: 2.4183 4.3179 sec/batch\n",
      "Epoch 2/20  Iteration 263/3560 Training loss: 2.4165 4.3083 sec/batch\n",
      "Epoch 2/20  Iteration 264/3560 Training loss: 2.4150 3.9166 sec/batch\n",
      "Epoch 2/20  Iteration 265/3560 Training loss: 2.4138 3.9895 sec/batch\n",
      "Epoch 2/20  Iteration 266/3560 Training loss: 2.4124 4.7470 sec/batch\n",
      "Epoch 2/20  Iteration 267/3560 Training loss: 2.4108 4.6558 sec/batch\n",
      "Epoch 2/20  Iteration 268/3560 Training loss: 2.4096 4.1378 sec/batch\n",
      "Epoch 2/20  Iteration 269/3560 Training loss: 2.4082 3.8258 sec/batch\n",
      "Epoch 2/20  Iteration 270/3560 Training loss: 2.4071 4.1057 sec/batch\n",
      "Epoch 2/20  Iteration 271/3560 Training loss: 2.4057 3.8455 sec/batch\n",
      "Epoch 2/20  Iteration 272/3560 Training loss: 2.4044 4.1257 sec/batch\n",
      "Epoch 2/20  Iteration 273/3560 Training loss: 2.4030 5.0184 sec/batch\n",
      "Epoch 2/20  Iteration 274/3560 Training loss: 2.4016 4.4147 sec/batch\n",
      "Epoch 2/20  Iteration 275/3560 Training loss: 2.4003 4.2585 sec/batch\n",
      "Epoch 2/20  Iteration 276/3560 Training loss: 2.3989 4.4301 sec/batch\n",
      "Epoch 2/20  Iteration 277/3560 Training loss: 2.3974 3.9988 sec/batch\n",
      "Epoch 2/20  Iteration 278/3560 Training loss: 2.3960 3.9607 sec/batch\n",
      "Epoch 2/20  Iteration 279/3560 Training loss: 2.3949 4.6398 sec/batch\n",
      "Epoch 2/20  Iteration 280/3560 Training loss: 2.3938 4.7167 sec/batch\n",
      "Epoch 2/20  Iteration 281/3560 Training loss: 2.3924 4.1960 sec/batch\n",
      "Epoch 2/20  Iteration 282/3560 Training loss: 2.3911 4.4874 sec/batch\n",
      "Epoch 2/20  Iteration 283/3560 Training loss: 2.3898 4.1434 sec/batch\n",
      "Epoch 2/20  Iteration 284/3560 Training loss: 2.3886 3.8434 sec/batch\n",
      "Epoch 2/20  Iteration 285/3560 Training loss: 2.3874 3.6087 sec/batch\n",
      "Epoch 2/20  Iteration 286/3560 Training loss: 2.3865 3.8044 sec/batch\n",
      "Epoch 2/20  Iteration 287/3560 Training loss: 2.3854 4.4308 sec/batch\n",
      "Epoch 2/20  Iteration 288/3560 Training loss: 2.3841 4.1509 sec/batch\n",
      "Epoch 2/20  Iteration 289/3560 Training loss: 2.3830 3.8271 sec/batch\n",
      "Epoch 2/20  Iteration 290/3560 Training loss: 2.3819 4.1035 sec/batch\n",
      "Epoch 2/20  Iteration 291/3560 Training loss: 2.3807 5.1565 sec/batch\n",
      "Epoch 2/20  Iteration 292/3560 Training loss: 2.3794 4.4065 sec/batch\n",
      "Epoch 2/20  Iteration 293/3560 Training loss: 2.3781 4.0204 sec/batch\n",
      "Epoch 2/20  Iteration 294/3560 Training loss: 2.3767 3.8999 sec/batch\n",
      "Epoch 2/20  Iteration 295/3560 Training loss: 2.3755 4.0507 sec/batch\n",
      "Epoch 2/20  Iteration 296/3560 Training loss: 2.3743 3.7654 sec/batch\n",
      "Epoch 2/20  Iteration 297/3560 Training loss: 2.3733 3.6452 sec/batch\n",
      "Epoch 2/20  Iteration 298/3560 Training loss: 2.3722 3.9249 sec/batch\n",
      "Epoch 2/20  Iteration 299/3560 Training loss: 2.3713 4.4775 sec/batch\n",
      "Epoch 2/20  Iteration 300/3560 Training loss: 2.3701 4.6951 sec/batch\n",
      "Epoch 2/20  Iteration 301/3560 Training loss: 2.3689 4.7143 sec/batch\n",
      "Epoch 2/20  Iteration 302/3560 Training loss: 2.3679 4.0258 sec/batch\n",
      "Epoch 2/20  Iteration 303/3560 Training loss: 2.3666 3.8327 sec/batch\n",
      "Epoch 2/20  Iteration 304/3560 Training loss: 2.3653 3.7602 sec/batch\n",
      "Epoch 2/20  Iteration 305/3560 Training loss: 2.3642 5.3669 sec/batch\n",
      "Epoch 2/20  Iteration 306/3560 Training loss: 2.3631 4.4180 sec/batch\n",
      "Epoch 2/20  Iteration 307/3560 Training loss: 2.3621 4.1682 sec/batch\n",
      "Epoch 2/20  Iteration 308/3560 Training loss: 2.3610 4.2762 sec/batch\n",
      "Epoch 2/20  Iteration 309/3560 Training loss: 2.3597 5.0749 sec/batch\n",
      "Epoch 2/20  Iteration 310/3560 Training loss: 2.3584 4.3632 sec/batch\n",
      "Epoch 2/20  Iteration 311/3560 Training loss: 2.3573 4.0003 sec/batch\n",
      "Epoch 2/20  Iteration 312/3560 Training loss: 2.3562 3.9816 sec/batch\n",
      "Epoch 2/20  Iteration 313/3560 Training loss: 2.3551 4.6155 sec/batch\n",
      "Epoch 2/20  Iteration 314/3560 Training loss: 2.3541 4.7552 sec/batch\n",
      "Epoch 2/20  Iteration 315/3560 Training loss: 2.3530 4.2208 sec/batch\n",
      "Epoch 2/20  Iteration 316/3560 Training loss: 2.3519 3.8876 sec/batch\n",
      "Epoch 2/20  Iteration 317/3560 Training loss: 2.3511 4.0941 sec/batch\n",
      "Epoch 2/20  Iteration 318/3560 Training loss: 2.3499 3.8722 sec/batch\n",
      "Epoch 2/20  Iteration 319/3560 Training loss: 2.3489 4.0784 sec/batch\n",
      "Epoch 2/20  Iteration 320/3560 Training loss: 2.3478 4.9370 sec/batch\n",
      "Epoch 2/20  Iteration 321/3560 Training loss: 2.3466 4.4957 sec/batch\n",
      "Epoch 2/20  Iteration 322/3560 Training loss: 2.3455 4.2213 sec/batch\n",
      "Epoch 2/20  Iteration 323/3560 Training loss: 2.3444 4.4563 sec/batch\n",
      "Epoch 2/20  Iteration 324/3560 Training loss: 2.3436 4.0307 sec/batch\n",
      "Epoch 2/20  Iteration 325/3560 Training loss: 2.3425 3.9206 sec/batch\n",
      "Epoch 2/20  Iteration 326/3560 Training loss: 2.3416 4.4860 sec/batch\n",
      "Epoch 2/20  Iteration 327/3560 Training loss: 2.3405 4.8198 sec/batch\n",
      "Epoch 2/20  Iteration 328/3560 Training loss: 2.3393 4.2724 sec/batch\n",
      "Epoch 2/20  Iteration 329/3560 Training loss: 2.3384 3.9334 sec/batch\n",
      "Epoch 2/20  Iteration 330/3560 Training loss: 2.3376 4.0687 sec/batch\n",
      "Epoch 2/20  Iteration 331/3560 Training loss: 2.3367 3.9200 sec/batch\n",
      "Epoch 2/20  Iteration 332/3560 Training loss: 2.3357 4.0624 sec/batch\n",
      "Epoch 2/20  Iteration 333/3560 Training loss: 2.3345 4.9494 sec/batch\n",
      "Epoch 2/20  Iteration 334/3560 Training loss: 2.3335 4.5509 sec/batch\n",
      "Epoch 2/20  Iteration 335/3560 Training loss: 2.3324 4.1079 sec/batch\n",
      "Epoch 2/20  Iteration 336/3560 Training loss: 2.3314 3.8611 sec/batch\n",
      "Epoch 2/20  Iteration 337/3560 Training loss: 2.3301 4.1270 sec/batch\n",
      "Epoch 2/20  Iteration 338/3560 Training loss: 2.3293 3.8476 sec/batch\n",
      "Epoch 2/20  Iteration 339/3560 Training loss: 2.3284 4.1265 sec/batch\n",
      "Epoch 2/20  Iteration 340/3560 Training loss: 2.3273 3.8866 sec/batch\n",
      "Epoch 2/20  Iteration 341/3560 Training loss: 2.3263 4.2434 sec/batch\n",
      "Epoch 2/20  Iteration 342/3560 Training loss: 2.3253 5.0043 sec/batch\n",
      "Epoch 2/20  Iteration 343/3560 Training loss: 2.3244 4.3838 sec/batch\n",
      "Epoch 2/20  Iteration 344/3560 Training loss: 2.3234 3.9920 sec/batch\n",
      "Epoch 2/20  Iteration 345/3560 Training loss: 2.3225 3.9748 sec/batch\n",
      "Epoch 2/20  Iteration 346/3560 Training loss: 2.3216 4.6286 sec/batch\n",
      "Epoch 2/20  Iteration 347/3560 Training loss: 2.3206 4.7376 sec/batch\n",
      "Epoch 2/20  Iteration 348/3560 Training loss: 2.3196 4.2071 sec/batch\n",
      "Epoch 2/20  Iteration 349/3560 Training loss: 2.3185 3.8915 sec/batch\n",
      "Epoch 2/20  Iteration 350/3560 Training loss: 2.3175 4.2909 sec/batch\n",
      "Epoch 2/20  Iteration 351/3560 Training loss: 2.3166 3.9339 sec/batch\n",
      "Epoch 2/20  Iteration 352/3560 Training loss: 2.3159 4.3850 sec/batch\n",
      "Epoch 2/20  Iteration 353/3560 Training loss: 2.3152 4.4684 sec/batch\n",
      "Epoch 2/20  Iteration 354/3560 Training loss: 2.3144 4.8098 sec/batch\n",
      "Epoch 2/20  Iteration 355/3560 Training loss: 2.3134 4.2630 sec/batch\n",
      "Epoch 2/20  Iteration 356/3560 Training loss: 2.3125 4.4383 sec/batch\n",
      "Epoch 3/20  Iteration 357/3560 Training loss: 2.2143 3.8908 sec/batch\n",
      "Epoch 3/20  Iteration 358/3560 Training loss: 2.1644 3.9565 sec/batch\n",
      "Epoch 3/20  Iteration 359/3560 Training loss: 2.1469 4.6608 sec/batch\n",
      "Epoch 3/20  Iteration 360/3560 Training loss: 2.1397 4.4449 sec/batch\n",
      "Epoch 3/20  Iteration 361/3560 Training loss: 2.1361 4.2173 sec/batch\n",
      "Epoch 3/20  Iteration 362/3560 Training loss: 2.1299 4.3862 sec/batch\n",
      "Epoch 3/20  Iteration 363/3560 Training loss: 2.1310 3.9869 sec/batch\n",
      "Epoch 3/20  Iteration 364/3560 Training loss: 2.1303 3.7610 sec/batch\n",
      "Epoch 3/20  Iteration 365/3560 Training loss: 2.1320 4.0400 sec/batch\n",
      "Epoch 3/20  Iteration 366/3560 Training loss: 2.1313 3.7385 sec/batch\n",
      "Epoch 3/20  Iteration 367/3560 Training loss: 2.1283 3.5565 sec/batch\n",
      "Epoch 3/20  Iteration 368/3560 Training loss: 2.1253 3.7189 sec/batch\n",
      "Epoch 3/20  Iteration 369/3560 Training loss: 2.1247 4.2911 sec/batch\n",
      "Epoch 3/20  Iteration 370/3560 Training loss: 2.1263 4.0823 sec/batch\n",
      "Epoch 3/20  Iteration 371/3560 Training loss: 2.1247 3.7814 sec/batch\n",
      "Epoch 3/20  Iteration 372/3560 Training loss: 2.1228 3.9945 sec/batch\n",
      "Epoch 3/20  Iteration 373/3560 Training loss: 2.1213 3.7761 sec/batch\n",
      "Epoch 3/20  Iteration 374/3560 Training loss: 2.1226 3.9391 sec/batch\n",
      "Epoch 3/20  Iteration 375/3560 Training loss: 2.1220 3.8236 sec/batch\n",
      "Epoch 3/20  Iteration 376/3560 Training loss: 2.1211 3.5850 sec/batch\n",
      "Epoch 3/20  Iteration 377/3560 Training loss: 2.1197 3.6614 sec/batch\n",
      "Epoch 3/20  Iteration 378/3560 Training loss: 2.1209 3.6330 sec/batch\n",
      "Epoch 3/20  Iteration 379/3560 Training loss: 2.1201 3.5043 sec/batch\n",
      "Epoch 3/20  Iteration 380/3560 Training loss: 2.1185 3.4690 sec/batch\n",
      "Epoch 3/20  Iteration 381/3560 Training loss: 2.1174 3.5933 sec/batch\n",
      "Epoch 3/20  Iteration 382/3560 Training loss: 2.1155 3.9833 sec/batch\n",
      "Epoch 3/20  Iteration 383/3560 Training loss: 2.1142 3.7345 sec/batch\n",
      "Epoch 3/20  Iteration 384/3560 Training loss: 2.1136 3.5288 sec/batch\n",
      "Epoch 3/20  Iteration 385/3560 Training loss: 2.1139 3.7193 sec/batch\n",
      "Epoch 3/20  Iteration 386/3560 Training loss: 2.1133 3.5634 sec/batch\n",
      "Epoch 3/20  Iteration 387/3560 Training loss: 2.1126 3.6783 sec/batch\n",
      "Epoch 3/20  Iteration 388/3560 Training loss: 2.1111 4.1522 sec/batch\n",
      "Epoch 3/20  Iteration 389/3560 Training loss: 2.1102 4.1264 sec/batch\n",
      "Epoch 3/20  Iteration 390/3560 Training loss: 2.1103 3.8205 sec/batch\n",
      "Epoch 3/20  Iteration 391/3560 Training loss: 2.1095 3.5955 sec/batch\n",
      "Epoch 3/20  Iteration 392/3560 Training loss: 2.1084 3.6718 sec/batch\n",
      "Epoch 3/20  Iteration 393/3560 Training loss: 2.1076 3.6686 sec/batch\n",
      "Epoch 3/20  Iteration 394/3560 Training loss: 2.1055 3.5869 sec/batch\n",
      "Epoch 3/20  Iteration 395/3560 Training loss: 2.1037 3.7037 sec/batch\n",
      "Epoch 3/20  Iteration 396/3560 Training loss: 2.1023 3.5524 sec/batch\n",
      "Epoch 3/20  Iteration 397/3560 Training loss: 2.1010 3.7609 sec/batch\n",
      "Epoch 3/20  Iteration 398/3560 Training loss: 2.1003 3.5274 sec/batch\n",
      "Epoch 3/20  Iteration 399/3560 Training loss: 2.0988 3.7231 sec/batch\n",
      "Epoch 3/20  Iteration 400/3560 Training loss: 2.0974 3.5745 sec/batch\n",
      "Validation loss: 1.95854 Saving checkpoint!\n",
      "Epoch 3/20  Iteration 401/3560 Training loss: 2.0975 3.5972 sec/batch\n",
      "Epoch 3/20  Iteration 402/3560 Training loss: 2.0953 3.5647 sec/batch\n",
      "Epoch 3/20  Iteration 403/3560 Training loss: 2.0947 3.7170 sec/batch\n",
      "Epoch 3/20  Iteration 404/3560 Training loss: 2.0934 3.5284 sec/batch\n",
      "Epoch 3/20  Iteration 405/3560 Training loss: 2.0925 3.4564 sec/batch\n",
      "Epoch 3/20  Iteration 406/3560 Training loss: 2.0923 3.5200 sec/batch\n",
      "Epoch 3/20  Iteration 407/3560 Training loss: 2.0910 3.9734 sec/batch\n",
      "Epoch 3/20  Iteration 408/3560 Training loss: 2.0910 3.7616 sec/batch\n",
      "Epoch 3/20  Iteration 409/3560 Training loss: 2.0900 3.6022 sec/batch\n",
      "Epoch 3/20  Iteration 410/3560 Training loss: 2.0891 3.6529 sec/batch\n",
      "Epoch 3/20  Iteration 411/3560 Training loss: 2.0881 4.0511 sec/batch\n",
      "Epoch 3/20  Iteration 412/3560 Training loss: 2.0875 5.0853 sec/batch\n",
      "Epoch 3/20  Iteration 413/3560 Training loss: 2.0870 5.0222 sec/batch\n",
      "Epoch 3/20  Iteration 414/3560 Training loss: 2.0862 4.3407 sec/batch\n",
      "Epoch 3/20  Iteration 415/3560 Training loss: 2.0851 4.0349 sec/batch\n",
      "Epoch 3/20  Iteration 416/3560 Training loss: 2.0850 4.3673 sec/batch\n",
      "Epoch 3/20  Iteration 417/3560 Training loss: 2.0842 4.1214 sec/batch\n",
      "Epoch 3/20  Iteration 418/3560 Training loss: 2.0840 4.7721 sec/batch\n",
      "Epoch 3/20  Iteration 419/3560 Training loss: 2.0836 5.2018 sec/batch\n",
      "Epoch 3/20  Iteration 420/3560 Training loss: 2.0830 4.4709 sec/batch\n",
      "Epoch 3/20  Iteration 421/3560 Training loss: 2.0820 4.0760 sec/batch\n",
      "Epoch 3/20  Iteration 422/3560 Training loss: 2.0818 4.3418 sec/batch\n",
      "Epoch 3/20  Iteration 423/3560 Training loss: 2.0812 5.4101 sec/batch\n",
      "Epoch 3/20  Iteration 424/3560 Training loss: 2.0800 4.6971 sec/batch\n",
      "Epoch 3/20  Iteration 425/3560 Training loss: 2.0791 4.1982 sec/batch\n",
      "Epoch 3/20  Iteration 426/3560 Training loss: 2.0786 4.2044 sec/batch\n",
      "Epoch 3/20  Iteration 427/3560 Training loss: 2.0784 5.1933 sec/batch\n",
      "Epoch 3/20  Iteration 428/3560 Training loss: 2.0776 4.9272 sec/batch\n",
      "Epoch 3/20  Iteration 429/3560 Training loss: 2.0772 4.4605 sec/batch\n",
      "Epoch 3/20  Iteration 430/3560 Training loss: 2.0761 4.8722 sec/batch\n",
      "Epoch 3/20  Iteration 431/3560 Training loss: 2.0753 4.6463 sec/batch\n",
      "Epoch 3/20  Iteration 432/3560 Training loss: 2.0750 4.5619 sec/batch\n",
      "Epoch 3/20  Iteration 433/3560 Training loss: 2.0742 4.8162 sec/batch\n",
      "Epoch 3/20  Iteration 434/3560 Training loss: 2.0737 6.1422 sec/batch\n",
      "Epoch 3/20  Iteration 435/3560 Training loss: 2.0726 5.1965 sec/batch\n",
      "Epoch 3/20  Iteration 436/3560 Training loss: 2.0717 4.4623 sec/batch\n",
      "Epoch 3/20  Iteration 437/3560 Training loss: 2.0706 4.8314 sec/batch\n",
      "Epoch 3/20  Iteration 438/3560 Training loss: 2.0700 4.4570 sec/batch\n",
      "Epoch 3/20  Iteration 439/3560 Training loss: 2.0689 5.5186 sec/batch\n",
      "Epoch 3/20  Iteration 440/3560 Training loss: 2.0681 5.8027 sec/batch\n",
      "Epoch 3/20  Iteration 441/3560 Training loss: 2.0669 5.1977 sec/batch\n",
      "Epoch 3/20  Iteration 442/3560 Training loss: 2.0659 5.1997 sec/batch\n",
      "Epoch 3/20  Iteration 443/3560 Training loss: 2.0652 4.5285 sec/batch\n",
      "Epoch 3/20  Iteration 444/3560 Training loss: 2.0642 4.8086 sec/batch\n",
      "Epoch 3/20  Iteration 445/3560 Training loss: 2.0631 6.1536 sec/batch\n",
      "Epoch 3/20  Iteration 446/3560 Training loss: 2.0625 5.1878 sec/batch\n",
      "Epoch 3/20  Iteration 447/3560 Training loss: 2.0616 5.4348 sec/batch\n",
      "Epoch 3/20  Iteration 448/3560 Training loss: 2.0609 5.0975 sec/batch\n",
      "Epoch 3/20  Iteration 449/3560 Training loss: 2.0598 5.2114 sec/batch\n",
      "Epoch 3/20  Iteration 450/3560 Training loss: 2.0589 5.4333 sec/batch\n",
      "Epoch 3/20  Iteration 451/3560 Training loss: 2.0580 5.0757 sec/batch\n",
      "Epoch 3/20  Iteration 452/3560 Training loss: 2.0573 5.2443 sec/batch\n",
      "Epoch 3/20  Iteration 453/3560 Training loss: 2.0565 4.5153 sec/batch\n",
      "Epoch 3/20  Iteration 454/3560 Training loss: 2.0556 4.8343 sec/batch\n",
      "Epoch 3/20  Iteration 455/3560 Training loss: 2.0545 4.4136 sec/batch\n",
      "Epoch 3/20  Iteration 456/3560 Training loss: 2.0535 5.2940 sec/batch\n",
      "Epoch 3/20  Iteration 457/3560 Training loss: 2.0528 6.0101 sec/batch\n",
      "Epoch 3/20  Iteration 458/3560 Training loss: 2.0522 5.0086 sec/batch\n",
      "Epoch 3/20  Iteration 459/3560 Training loss: 2.0513 5.3474 sec/batch\n",
      "Epoch 3/20  Iteration 460/3560 Training loss: 2.0505 5.3374 sec/batch\n",
      "Epoch 3/20  Iteration 461/3560 Training loss: 2.0496 5.0227 sec/batch\n",
      "Epoch 3/20  Iteration 462/3560 Training loss: 2.0488 5.4074 sec/batch\n",
      "Epoch 3/20  Iteration 463/3560 Training loss: 2.0480 5.3021 sec/batch\n",
      "Epoch 3/20  Iteration 464/3560 Training loss: 2.0474 6.8805 sec/batch\n",
      "Epoch 3/20  Iteration 465/3560 Training loss: 2.0468 6.4995 sec/batch\n",
      "Epoch 3/20  Iteration 466/3560 Training loss: 2.0459 5.5965 sec/batch\n",
      "Epoch 3/20  Iteration 467/3560 Training loss: 2.0452 5.5952 sec/batch\n",
      "Epoch 3/20  Iteration 468/3560 Training loss: 2.0445 6.0638 sec/batch\n",
      "Epoch 3/20  Iteration 469/3560 Training loss: 2.0437 6.7248 sec/batch\n",
      "Epoch 3/20  Iteration 470/3560 Training loss: 2.0430 5.8079 sec/batch\n",
      "Epoch 3/20  Iteration 471/3560 Training loss: 2.0421 4.5704 sec/batch\n",
      "Epoch 3/20  Iteration 472/3560 Training loss: 2.0411 4.8417 sec/batch\n",
      "Epoch 3/20  Iteration 473/3560 Training loss: 2.0405 4.7319 sec/batch\n",
      "Epoch 3/20  Iteration 474/3560 Training loss: 2.0398 5.5804 sec/batch\n",
      "Epoch 3/20  Iteration 475/3560 Training loss: 2.0392 5.4183 sec/batch\n",
      "Epoch 3/20  Iteration 476/3560 Training loss: 2.0385 5.3399 sec/batch\n",
      "Epoch 3/20  Iteration 477/3560 Training loss: 2.0380 5.5563 sec/batch\n",
      "Epoch 3/20  Iteration 478/3560 Training loss: 2.0370 4.9496 sec/batch\n",
      "Epoch 3/20  Iteration 479/3560 Training loss: 2.0361 4.6346 sec/batch\n",
      "Epoch 3/20  Iteration 480/3560 Training loss: 2.0356 5.3469 sec/batch\n",
      "Epoch 3/20  Iteration 481/3560 Training loss: 2.0349 6.6472 sec/batch\n",
      "Epoch 3/20  Iteration 482/3560 Training loss: 2.0339 5.6503 sec/batch\n",
      "Epoch 3/20  Iteration 483/3560 Training loss: 2.0335 5.5418 sec/batch\n",
      "Epoch 3/20  Iteration 484/3560 Training loss: 2.0328 5.0029 sec/batch\n",
      "Epoch 3/20  Iteration 485/3560 Training loss: 2.0322 4.2482 sec/batch\n",
      "Epoch 3/20  Iteration 486/3560 Training loss: 2.0316 4.9936 sec/batch\n",
      "Epoch 3/20  Iteration 487/3560 Training loss: 2.0307 6.0076 sec/batch\n",
      "Epoch 3/20  Iteration 488/3560 Training loss: 2.0299 5.9662 sec/batch\n",
      "Epoch 3/20  Iteration 489/3560 Training loss: 2.0293 5.1343 sec/batch\n",
      "Epoch 3/20  Iteration 490/3560 Training loss: 2.0287 5.1375 sec/batch\n",
      "Epoch 3/20  Iteration 491/3560 Training loss: 2.0280 4.5322 sec/batch\n",
      "Epoch 3/20  Iteration 492/3560 Training loss: 2.0275 4.2415 sec/batch\n",
      "Epoch 3/20  Iteration 493/3560 Training loss: 2.0269 5.2742 sec/batch\n",
      "Epoch 3/20  Iteration 494/3560 Training loss: 2.0263 5.2083 sec/batch\n",
      "Epoch 3/20  Iteration 495/3560 Training loss: 2.0260 4.5771 sec/batch\n",
      "Epoch 3/20  Iteration 496/3560 Training loss: 2.0253 4.9703 sec/batch\n",
      "Epoch 3/20  Iteration 497/3560 Training loss: 2.0248 6.4831 sec/batch\n",
      "Epoch 3/20  Iteration 498/3560 Training loss: 2.0242 5.4417 sec/batch\n",
      "Epoch 3/20  Iteration 499/3560 Training loss: 2.0236 4.5010 sec/batch\n",
      "Epoch 3/20  Iteration 500/3560 Training loss: 2.0230 4.3924 sec/batch\n",
      "Epoch 3/20  Iteration 501/3560 Training loss: 2.0223 5.2628 sec/batch\n",
      "Epoch 3/20  Iteration 502/3560 Training loss: 2.0217 5.1341 sec/batch\n",
      "Epoch 3/20  Iteration 503/3560 Training loss: 2.0212 6.5041 sec/batch\n",
      "Epoch 3/20  Iteration 504/3560 Training loss: 2.0207 5.3522 sec/batch\n",
      "Epoch 3/20  Iteration 505/3560 Training loss: 2.0201 4.8440 sec/batch\n",
      "Epoch 3/20  Iteration 506/3560 Training loss: 2.0193 4.7487 sec/batch\n",
      "Epoch 3/20  Iteration 507/3560 Training loss: 2.0187 5.5107 sec/batch\n",
      "Epoch 3/20  Iteration 508/3560 Training loss: 2.0183 5.6032 sec/batch\n",
      "Epoch 3/20  Iteration 509/3560 Training loss: 2.0178 5.1364 sec/batch\n",
      "Epoch 3/20  Iteration 510/3560 Training loss: 2.0172 4.8313 sec/batch\n",
      "Epoch 3/20  Iteration 511/3560 Training loss: 2.0165 4.7869 sec/batch\n",
      "Epoch 3/20  Iteration 512/3560 Training loss: 2.0159 4.2541 sec/batch\n",
      "Epoch 3/20  Iteration 513/3560 Training loss: 2.0152 4.3291 sec/batch\n",
      "Epoch 3/20  Iteration 514/3560 Training loss: 2.0146 5.4354 sec/batch\n",
      "Epoch 3/20  Iteration 515/3560 Training loss: 2.0139 4.8195 sec/batch\n",
      "Epoch 3/20  Iteration 516/3560 Training loss: 2.0135 4.8518 sec/batch\n",
      "Epoch 3/20  Iteration 517/3560 Training loss: 2.0131 5.6341 sec/batch\n",
      "Epoch 3/20  Iteration 518/3560 Training loss: 2.0125 4.1064 sec/batch\n",
      "Epoch 3/20  Iteration 519/3560 Training loss: 2.0120 4.5961 sec/batch\n",
      "Epoch 3/20  Iteration 520/3560 Training loss: 2.0115 5.7810 sec/batch\n",
      "Epoch 3/20  Iteration 521/3560 Training loss: 2.0109 5.1731 sec/batch\n",
      "Epoch 3/20  Iteration 522/3560 Training loss: 2.0103 4.4033 sec/batch\n",
      "Epoch 3/20  Iteration 523/3560 Training loss: 2.0098 3.8812 sec/batch\n",
      "Epoch 3/20  Iteration 524/3560 Training loss: 2.0096 4.1416 sec/batch\n",
      "Epoch 3/20  Iteration 525/3560 Training loss: 2.0090 4.9232 sec/batch\n",
      "Epoch 3/20  Iteration 526/3560 Training loss: 2.0084 4.5771 sec/batch\n",
      "Epoch 3/20  Iteration 527/3560 Training loss: 2.0077 5.0001 sec/batch\n",
      "Epoch 3/20  Iteration 528/3560 Training loss: 2.0071 4.7604 sec/batch\n",
      "Epoch 3/20  Iteration 529/3560 Training loss: 2.0065 4.4903 sec/batch\n",
      "Epoch 3/20  Iteration 530/3560 Training loss: 2.0060 4.9010 sec/batch\n",
      "Epoch 3/20  Iteration 531/3560 Training loss: 2.0055 4.6794 sec/batch\n",
      "Epoch 3/20  Iteration 532/3560 Training loss: 2.0049 4.3900 sec/batch\n",
      "Epoch 3/20  Iteration 533/3560 Training loss: 2.0042 5.6426 sec/batch\n",
      "Epoch 3/20  Iteration 534/3560 Training loss: 2.0037 5.2635 sec/batch\n",
      "Epoch 4/20  Iteration 535/3560 Training loss: 1.9736 4.5475 sec/batch\n",
      "Epoch 4/20  Iteration 536/3560 Training loss: 1.9252 4.8725 sec/batch\n",
      "Epoch 4/20  Iteration 537/3560 Training loss: 1.9131 5.0558 sec/batch\n",
      "Epoch 4/20  Iteration 538/3560 Training loss: 1.9040 4.6417 sec/batch\n",
      "Epoch 4/20  Iteration 539/3560 Training loss: 1.9011 4.9145 sec/batch\n",
      "Epoch 4/20  Iteration 540/3560 Training loss: 1.8901 4.5836 sec/batch\n",
      "Epoch 4/20  Iteration 541/3560 Training loss: 1.8911 4.8513 sec/batch\n",
      "Epoch 4/20  Iteration 542/3560 Training loss: 1.8894 4.4729 sec/batch\n",
      "Epoch 4/20  Iteration 543/3560 Training loss: 1.8930 4.2763 sec/batch\n",
      "Epoch 4/20  Iteration 544/3560 Training loss: 1.8927 5.3335 sec/batch\n",
      "Epoch 4/20  Iteration 545/3560 Training loss: 1.8895 5.3433 sec/batch\n",
      "Epoch 4/20  Iteration 546/3560 Training loss: 1.8875 5.7801 sec/batch\n",
      "Epoch 4/20  Iteration 547/3560 Training loss: 1.8876 4.7685 sec/batch\n",
      "Epoch 4/20  Iteration 548/3560 Training loss: 1.8903 4.3163 sec/batch\n",
      "Epoch 4/20  Iteration 549/3560 Training loss: 1.8894 5.1029 sec/batch\n",
      "Epoch 4/20  Iteration 550/3560 Training loss: 1.8875 5.7685 sec/batch\n",
      "Epoch 4/20  Iteration 551/3560 Training loss: 1.8865 4.6959 sec/batch\n",
      "Epoch 4/20  Iteration 552/3560 Training loss: 1.8882 5.6912 sec/batch\n",
      "Epoch 4/20  Iteration 553/3560 Training loss: 1.8879 4.7098 sec/batch\n",
      "Epoch 4/20  Iteration 554/3560 Training loss: 1.8880 4.6169 sec/batch\n",
      "Epoch 4/20  Iteration 555/3560 Training loss: 1.8872 4.7971 sec/batch\n",
      "Epoch 4/20  Iteration 556/3560 Training loss: 1.8884 4.4292 sec/batch\n",
      "Epoch 4/20  Iteration 557/3560 Training loss: 1.8873 4.9300 sec/batch\n",
      "Epoch 4/20  Iteration 558/3560 Training loss: 1.8865 4.6294 sec/batch\n",
      "Epoch 4/20  Iteration 559/3560 Training loss: 1.8858 4.7710 sec/batch\n",
      "Epoch 4/20  Iteration 560/3560 Training loss: 1.8841 4.5264 sec/batch\n",
      "Epoch 4/20  Iteration 561/3560 Training loss: 1.8827 4.2819 sec/batch\n",
      "Epoch 4/20  Iteration 562/3560 Training loss: 1.8826 5.9171 sec/batch\n",
      "Epoch 4/20  Iteration 563/3560 Training loss: 1.8830 5.5352 sec/batch\n",
      "Epoch 4/20  Iteration 564/3560 Training loss: 1.8830 4.6613 sec/batch\n",
      "Epoch 4/20  Iteration 565/3560 Training loss: 1.8824 4.5234 sec/batch\n",
      "Epoch 4/20  Iteration 566/3560 Training loss: 1.8811 5.6957 sec/batch\n",
      "Epoch 4/20  Iteration 567/3560 Training loss: 1.8809 5.0427 sec/batch\n",
      "Epoch 4/20  Iteration 568/3560 Training loss: 1.8810 4.1617 sec/batch\n",
      "Epoch 4/20  Iteration 569/3560 Training loss: 1.8802 4.2779 sec/batch\n",
      "Epoch 4/20  Iteration 570/3560 Training loss: 1.8794 5.3225 sec/batch\n",
      "Epoch 4/20  Iteration 571/3560 Training loss: 1.8786 4.9194 sec/batch\n",
      "Epoch 4/20  Iteration 572/3560 Training loss: 1.8771 4.4838 sec/batch\n",
      "Epoch 4/20  Iteration 573/3560 Training loss: 1.8754 4.5181 sec/batch\n",
      "Epoch 4/20  Iteration 574/3560 Training loss: 1.8743 5.0593 sec/batch\n",
      "Epoch 4/20  Iteration 575/3560 Training loss: 1.8732 5.6931 sec/batch\n",
      "Epoch 4/20  Iteration 576/3560 Training loss: 1.8729 5.4648 sec/batch\n",
      "Epoch 4/20  Iteration 577/3560 Training loss: 1.8720 6.0622 sec/batch\n",
      "Epoch 4/20  Iteration 578/3560 Training loss: 1.8706 5.5715 sec/batch\n",
      "Epoch 4/20  Iteration 579/3560 Training loss: 1.8703 5.0374 sec/batch\n",
      "Epoch 4/20  Iteration 580/3560 Training loss: 1.8687 4.8266 sec/batch\n",
      "Epoch 4/20  Iteration 581/3560 Training loss: 1.8681 5.0019 sec/batch\n",
      "Epoch 4/20  Iteration 582/3560 Training loss: 1.8672 4.9260 sec/batch\n",
      "Epoch 4/20  Iteration 583/3560 Training loss: 1.8666 4.7670 sec/batch\n",
      "Epoch 4/20  Iteration 584/3560 Training loss: 1.8668 5.4311 sec/batch\n",
      "Epoch 4/20  Iteration 585/3560 Training loss: 1.8658 5.3304 sec/batch\n",
      "Epoch 4/20  Iteration 586/3560 Training loss: 1.8662 4.5043 sec/batch\n",
      "Epoch 4/20  Iteration 587/3560 Training loss: 1.8657 5.7581 sec/batch\n",
      "Epoch 4/20  Iteration 588/3560 Training loss: 1.8652 5.1744 sec/batch\n",
      "Epoch 4/20  Iteration 589/3560 Training loss: 1.8646 4.5561 sec/batch\n",
      "Epoch 4/20  Iteration 590/3560 Training loss: 1.8643 6.8816 sec/batch\n",
      "Epoch 4/20  Iteration 591/3560 Training loss: 1.8642 5.0616 sec/batch\n",
      "Epoch 4/20  Iteration 592/3560 Training loss: 1.8635 4.7493 sec/batch\n",
      "Epoch 4/20  Iteration 593/3560 Training loss: 1.8627 4.7532 sec/batch\n",
      "Epoch 4/20  Iteration 594/3560 Training loss: 1.8628 4.4749 sec/batch\n",
      "Epoch 4/20  Iteration 595/3560 Training loss: 1.8624 4.6177 sec/batch\n",
      "Epoch 4/20  Iteration 596/3560 Training loss: 1.8628 6.2279 sec/batch\n",
      "Epoch 4/20  Iteration 597/3560 Training loss: 1.8626 4.9013 sec/batch\n",
      "Epoch 4/20  Iteration 598/3560 Training loss: 1.8625 4.5144 sec/batch\n",
      "Epoch 4/20  Iteration 599/3560 Training loss: 1.8620 4.7554 sec/batch\n",
      "Epoch 4/20  Iteration 600/3560 Training loss: 1.8620 4.5308 sec/batch\n",
      "Validation loss: 1.72109 Saving checkpoint!\n",
      "Epoch 4/20  Iteration 601/3560 Training loss: 1.8624 5.0167 sec/batch\n",
      "Epoch 4/20  Iteration 602/3560 Training loss: 1.8615 4.4250 sec/batch\n",
      "Epoch 4/20  Iteration 603/3560 Training loss: 1.8610 4.0151 sec/batch\n",
      "Epoch 4/20  Iteration 604/3560 Training loss: 1.8604 3.7278 sec/batch\n",
      "Epoch 4/20  Iteration 605/3560 Training loss: 1.8605 4.0109 sec/batch\n",
      "Epoch 4/20  Iteration 606/3560 Training loss: 1.8602 4.7789 sec/batch\n",
      "Epoch 4/20  Iteration 607/3560 Training loss: 1.8601 4.2906 sec/batch\n",
      "Epoch 4/20  Iteration 608/3560 Training loss: 1.8593 4.0661 sec/batch\n",
      "Epoch 4/20  Iteration 609/3560 Training loss: 1.8588 4.7049 sec/batch\n",
      "Epoch 4/20  Iteration 610/3560 Training loss: 1.8586 5.2166 sec/batch\n",
      "Epoch 4/20  Iteration 611/3560 Training loss: 1.8581 4.4891 sec/batch\n",
      "Epoch 4/20  Iteration 612/3560 Training loss: 1.8578 4.0695 sec/batch\n",
      "Epoch 4/20  Iteration 613/3560 Training loss: 1.8569 4.0186 sec/batch\n",
      "Epoch 4/20  Iteration 614/3560 Training loss: 1.8563 4.5764 sec/batch\n",
      "Epoch 4/20  Iteration 615/3560 Training loss: 1.8554 5.2919 sec/batch\n",
      "Epoch 4/20  Iteration 616/3560 Training loss: 1.8550 4.5451 sec/batch\n",
      "Epoch 4/20  Iteration 617/3560 Training loss: 1.8541 4.0994 sec/batch\n",
      "Epoch 4/20  Iteration 618/3560 Training loss: 1.8536 4.3015 sec/batch\n",
      "Epoch 4/20  Iteration 619/3560 Training loss: 1.8528 5.3309 sec/batch\n",
      "Epoch 4/20  Iteration 620/3560 Training loss: 1.8520 4.7053 sec/batch\n",
      "Epoch 4/20  Iteration 621/3560 Training loss: 1.8515 4.2381 sec/batch\n",
      "Epoch 4/20  Iteration 622/3560 Training loss: 1.8507 4.2982 sec/batch\n",
      "Epoch 4/20  Iteration 623/3560 Training loss: 1.8498 4.0964 sec/batch\n",
      "Epoch 4/20  Iteration 624/3560 Training loss: 1.8495 5.1048 sec/batch\n",
      "Epoch 4/20  Iteration 625/3560 Training loss: 1.8488 5.1209 sec/batch\n",
      "Epoch 4/20  Iteration 626/3560 Training loss: 1.8483 4.2624 sec/batch\n",
      "Epoch 4/20  Iteration 627/3560 Training loss: 1.8474 4.2467 sec/batch\n",
      "Epoch 4/20  Iteration 628/3560 Training loss: 1.8468 5.0633 sec/batch\n",
      "Epoch 4/20  Iteration 629/3560 Training loss: 1.8460 5.2341 sec/batch\n",
      "Epoch 4/20  Iteration 630/3560 Training loss: 1.8454 4.5729 sec/batch\n",
      "Epoch 4/20  Iteration 631/3560 Training loss: 1.8450 5.4090 sec/batch\n",
      "Epoch 4/20  Iteration 632/3560 Training loss: 1.8442 5.1983 sec/batch\n",
      "Epoch 4/20  Iteration 633/3560 Training loss: 1.8435 4.6420 sec/batch\n",
      "Epoch 4/20  Iteration 634/3560 Training loss: 1.8426 4.5565 sec/batch\n",
      "Epoch 4/20  Iteration 635/3560 Training loss: 1.8422 4.4499 sec/batch\n",
      "Epoch 4/20  Iteration 636/3560 Training loss: 1.8417 4.5932 sec/batch\n",
      "Epoch 4/20  Iteration 637/3560 Training loss: 1.8411 4.1370 sec/batch\n",
      "Epoch 4/20  Iteration 638/3560 Training loss: 1.8404 4.7373 sec/batch\n",
      "Epoch 4/20  Iteration 639/3560 Training loss: 1.8398 5.9122 sec/batch\n",
      "Epoch 4/20  Iteration 640/3560 Training loss: 1.8394 5.1784 sec/batch\n",
      "Epoch 4/20  Iteration 641/3560 Training loss: 1.8389 4.1883 sec/batch\n",
      "Epoch 4/20  Iteration 642/3560 Training loss: 1.8384 4.3663 sec/batch\n",
      "Epoch 4/20  Iteration 643/3560 Training loss: 1.8382 4.1821 sec/batch\n",
      "Epoch 4/20  Iteration 644/3560 Training loss: 1.8378 4.7648 sec/batch\n",
      "Epoch 4/20  Iteration 645/3560 Training loss: 1.8373 5.2902 sec/batch\n",
      "Epoch 4/20  Iteration 646/3560 Training loss: 1.8367 4.5442 sec/batch\n",
      "Epoch 4/20  Iteration 647/3560 Training loss: 1.8362 4.1394 sec/batch\n",
      "Epoch 4/20  Iteration 648/3560 Training loss: 1.8356 4.8223 sec/batch\n",
      "Epoch 4/20  Iteration 649/3560 Training loss: 1.8350 6.0431 sec/batch\n",
      "Epoch 4/20  Iteration 650/3560 Training loss: 1.8343 5.0182 sec/batch\n",
      "Epoch 4/20  Iteration 651/3560 Training loss: 1.8339 4.4022 sec/batch\n",
      "Epoch 4/20  Iteration 652/3560 Training loss: 1.8333 4.5981 sec/batch\n",
      "Epoch 4/20  Iteration 653/3560 Training loss: 1.8329 4.0672 sec/batch\n",
      "Epoch 4/20  Iteration 654/3560 Training loss: 1.8324 4.7341 sec/batch\n",
      "Epoch 4/20  Iteration 655/3560 Training loss: 1.8319 4.3773 sec/batch\n",
      "Epoch 4/20  Iteration 656/3560 Training loss: 1.8312 5.2261 sec/batch\n",
      "Epoch 4/20  Iteration 657/3560 Training loss: 1.8305 5.2711 sec/batch\n",
      "Epoch 4/20  Iteration 658/3560 Training loss: 1.8302 4.8900 sec/batch\n",
      "Epoch 4/20  Iteration 659/3560 Training loss: 1.8297 4.8609 sec/batch\n",
      "Epoch 4/20  Iteration 660/3560 Training loss: 1.8288 4.8202 sec/batch\n",
      "Epoch 4/20  Iteration 661/3560 Training loss: 1.8285 4.7913 sec/batch\n",
      "Epoch 4/20  Iteration 662/3560 Training loss: 1.8282 4.4484 sec/batch\n",
      "Epoch 4/20  Iteration 663/3560 Training loss: 1.8277 4.6249 sec/batch\n",
      "Epoch 4/20  Iteration 664/3560 Training loss: 1.8272 5.9310 sec/batch\n",
      "Epoch 4/20  Iteration 665/3560 Training loss: 1.8266 5.7815 sec/batch\n",
      "Epoch 4/20  Iteration 666/3560 Training loss: 1.8259 4.8784 sec/batch\n",
      "Epoch 4/20  Iteration 667/3560 Training loss: 1.8256 5.7963 sec/batch\n",
      "Epoch 4/20  Iteration 668/3560 Training loss: 1.8253 5.3424 sec/batch\n",
      "Epoch 4/20  Iteration 669/3560 Training loss: 1.8249 4.6938 sec/batch\n",
      "Epoch 4/20  Iteration 670/3560 Training loss: 1.8246 4.8452 sec/batch\n",
      "Epoch 4/20  Iteration 671/3560 Training loss: 1.8243 4.3098 sec/batch\n",
      "Epoch 4/20  Iteration 672/3560 Training loss: 1.8240 4.4180 sec/batch\n",
      "Epoch 4/20  Iteration 673/3560 Training loss: 1.8237 5.4431 sec/batch\n",
      "Epoch 4/20  Iteration 674/3560 Training loss: 1.8233 5.2137 sec/batch\n",
      "Epoch 4/20  Iteration 675/3560 Training loss: 1.8232 4.4585 sec/batch\n",
      "Epoch 4/20  Iteration 676/3560 Training loss: 1.8228 4.3746 sec/batch\n",
      "Epoch 4/20  Iteration 677/3560 Training loss: 1.8224 6.0671 sec/batch\n",
      "Epoch 4/20  Iteration 678/3560 Training loss: 1.8221 4.9576 sec/batch\n",
      "Epoch 4/20  Iteration 679/3560 Training loss: 1.8216 4.3121 sec/batch\n",
      "Epoch 4/20  Iteration 680/3560 Training loss: 1.8213 4.2639 sec/batch\n",
      "Epoch 4/20  Iteration 681/3560 Training loss: 1.8210 4.4032 sec/batch\n",
      "Epoch 4/20  Iteration 682/3560 Training loss: 1.8209 4.5324 sec/batch\n",
      "Epoch 4/20  Iteration 683/3560 Training loss: 1.8206 4.8918 sec/batch\n",
      "Epoch 4/20  Iteration 684/3560 Training loss: 1.8201 4.5830 sec/batch\n",
      "Epoch 4/20  Iteration 685/3560 Training loss: 1.8196 4.0998 sec/batch\n",
      "Epoch 4/20  Iteration 686/3560 Training loss: 1.8193 5.2324 sec/batch\n",
      "Epoch 4/20  Iteration 687/3560 Training loss: 1.8190 5.8491 sec/batch\n",
      "Epoch 4/20  Iteration 688/3560 Training loss: 1.8186 5.0500 sec/batch\n",
      "Epoch 4/20  Iteration 689/3560 Training loss: 1.8182 4.9404 sec/batch\n",
      "Epoch 4/20  Iteration 690/3560 Training loss: 1.8179 6.0671 sec/batch\n",
      "Epoch 4/20  Iteration 691/3560 Training loss: 1.8176 6.2988 sec/batch\n",
      "Epoch 4/20  Iteration 692/3560 Training loss: 1.8172 4.6707 sec/batch\n",
      "Epoch 4/20  Iteration 693/3560 Training loss: 1.8167 4.7060 sec/batch\n",
      "Epoch 4/20  Iteration 694/3560 Training loss: 1.8164 4.3944 sec/batch\n",
      "Epoch 4/20  Iteration 695/3560 Training loss: 1.8162 4.4419 sec/batch\n",
      "Epoch 4/20  Iteration 696/3560 Training loss: 1.8158 5.8556 sec/batch\n",
      "Epoch 4/20  Iteration 697/3560 Training loss: 1.8156 4.8369 sec/batch\n",
      "Epoch 4/20  Iteration 698/3560 Training loss: 1.8153 4.7714 sec/batch\n",
      "Epoch 4/20  Iteration 699/3560 Training loss: 1.8149 4.4691 sec/batch\n",
      "Epoch 4/20  Iteration 700/3560 Training loss: 1.8144 5.7611 sec/batch\n",
      "Epoch 4/20  Iteration 701/3560 Training loss: 1.8141 5.7493 sec/batch\n",
      "Epoch 4/20  Iteration 702/3560 Training loss: 1.8141 5.2683 sec/batch\n",
      "Epoch 4/20  Iteration 703/3560 Training loss: 1.8137 4.8323 sec/batch\n",
      "Epoch 4/20  Iteration 704/3560 Training loss: 1.8133 4.1151 sec/batch\n",
      "Epoch 4/20  Iteration 705/3560 Training loss: 1.8129 4.8223 sec/batch\n",
      "Epoch 4/20  Iteration 706/3560 Training loss: 1.8123 5.2436 sec/batch\n",
      "Epoch 4/20  Iteration 707/3560 Training loss: 1.8121 5.3603 sec/batch\n",
      "Epoch 4/20  Iteration 708/3560 Training loss: 1.8117 4.9412 sec/batch\n",
      "Epoch 4/20  Iteration 709/3560 Training loss: 1.8114 4.3549 sec/batch\n",
      "Epoch 4/20  Iteration 710/3560 Training loss: 1.8110 4.2528 sec/batch\n",
      "Epoch 4/20  Iteration 711/3560 Training loss: 1.8105 5.3074 sec/batch\n",
      "Epoch 4/20  Iteration 712/3560 Training loss: 1.8102 5.0059 sec/batch\n",
      "Epoch 5/20  Iteration 713/3560 Training loss: 1.8421 4.4517 sec/batch\n",
      "Epoch 5/20  Iteration 714/3560 Training loss: 1.7905 4.6913 sec/batch\n",
      "Epoch 5/20  Iteration 715/3560 Training loss: 1.7704 4.4243 sec/batch\n",
      "Epoch 5/20  Iteration 716/3560 Training loss: 1.7629 4.7184 sec/batch\n",
      "Epoch 5/20  Iteration 717/3560 Training loss: 1.7564 5.6351 sec/batch\n",
      "Epoch 5/20  Iteration 718/3560 Training loss: 1.7437 5.3112 sec/batch\n",
      "Epoch 5/20  Iteration 719/3560 Training loss: 1.7426 5.6302 sec/batch\n",
      "Epoch 5/20  Iteration 720/3560 Training loss: 1.7396 4.6233 sec/batch\n",
      "Epoch 5/20  Iteration 721/3560 Training loss: 1.7415 4.1577 sec/batch\n",
      "Epoch 5/20  Iteration 722/3560 Training loss: 1.7406 4.9425 sec/batch\n",
      "Epoch 5/20  Iteration 723/3560 Training loss: 1.7375 6.4598 sec/batch\n",
      "Epoch 5/20  Iteration 724/3560 Training loss: 1.7353 5.5210 sec/batch\n",
      "Epoch 5/20  Iteration 725/3560 Training loss: 1.7352 5.5478 sec/batch\n",
      "Epoch 5/20  Iteration 726/3560 Training loss: 1.7374 6.8767 sec/batch\n",
      "Epoch 5/20  Iteration 727/3560 Training loss: 1.7363 5.9960 sec/batch\n",
      "Epoch 5/20  Iteration 728/3560 Training loss: 1.7351 4.6307 sec/batch\n",
      "Epoch 5/20  Iteration 729/3560 Training loss: 1.7343 5.9498 sec/batch\n",
      "Epoch 5/20  Iteration 730/3560 Training loss: 1.7356 5.5399 sec/batch\n",
      "Epoch 5/20  Iteration 731/3560 Training loss: 1.7351 6.7178 sec/batch\n",
      "Epoch 5/20  Iteration 732/3560 Training loss: 1.7353 5.4130 sec/batch\n",
      "Epoch 5/20  Iteration 733/3560 Training loss: 1.7342 4.6236 sec/batch\n",
      "Epoch 5/20  Iteration 734/3560 Training loss: 1.7351 5.2461 sec/batch\n",
      "Epoch 5/20  Iteration 735/3560 Training loss: 1.7339 5.4351 sec/batch\n",
      "Epoch 5/20  Iteration 736/3560 Training loss: 1.7335 5.0785 sec/batch\n",
      "Epoch 5/20  Iteration 737/3560 Training loss: 1.7331 5.1117 sec/batch\n",
      "Epoch 5/20  Iteration 738/3560 Training loss: 1.7314 4.9516 sec/batch\n",
      "Epoch 5/20  Iteration 739/3560 Training loss: 1.7297 5.2391 sec/batch\n",
      "Epoch 5/20  Iteration 740/3560 Training loss: 1.7297 6.1420 sec/batch\n",
      "Epoch 5/20  Iteration 741/3560 Training loss: 1.7301 5.8094 sec/batch\n",
      "Epoch 5/20  Iteration 742/3560 Training loss: 1.7299 6.8903 sec/batch\n",
      "Epoch 5/20  Iteration 743/3560 Training loss: 1.7295 6.5087 sec/batch\n",
      "Epoch 5/20  Iteration 744/3560 Training loss: 1.7281 5.0455 sec/batch\n",
      "Epoch 5/20  Iteration 745/3560 Training loss: 1.7280 4.6514 sec/batch\n",
      "Epoch 5/20  Iteration 746/3560 Training loss: 1.7282 4.7703 sec/batch\n",
      "Epoch 5/20  Iteration 747/3560 Training loss: 1.7276 5.9918 sec/batch\n",
      "Epoch 5/20  Iteration 748/3560 Training loss: 1.7269 5.2624 sec/batch\n",
      "Epoch 5/20  Iteration 749/3560 Training loss: 1.7260 5.3634 sec/batch\n",
      "Epoch 5/20  Iteration 750/3560 Training loss: 1.7246 4.8568 sec/batch\n",
      "Epoch 5/20  Iteration 751/3560 Training loss: 1.7232 4.5040 sec/batch\n",
      "Epoch 5/20  Iteration 752/3560 Training loss: 1.7221 4.7005 sec/batch\n",
      "Epoch 5/20  Iteration 753/3560 Training loss: 1.7211 4.6391 sec/batch\n",
      "Epoch 5/20  Iteration 754/3560 Training loss: 1.7214 5.8867 sec/batch\n",
      "Epoch 5/20  Iteration 755/3560 Training loss: 1.7207 5.3662 sec/batch\n",
      "Epoch 5/20  Iteration 756/3560 Training loss: 1.7196 4.6282 sec/batch\n",
      "Epoch 5/20  Iteration 757/3560 Training loss: 1.7194 4.7184 sec/batch\n",
      "Epoch 5/20  Iteration 758/3560 Training loss: 1.7181 4.4754 sec/batch\n",
      "Epoch 5/20  Iteration 759/3560 Training loss: 1.7177 4.9958 sec/batch\n",
      "Epoch 5/20  Iteration 760/3560 Training loss: 1.7170 6.0767 sec/batch\n",
      "Epoch 5/20  Iteration 761/3560 Training loss: 1.7164 5.0740 sec/batch\n",
      "Epoch 5/20  Iteration 762/3560 Training loss: 1.7168 5.4182 sec/batch\n",
      "Epoch 5/20  Iteration 763/3560 Training loss: 1.7160 5.1524 sec/batch\n",
      "Epoch 5/20  Iteration 764/3560 Training loss: 1.7167 5.1226 sec/batch\n",
      "Epoch 5/20  Iteration 765/3560 Training loss: 1.7162 5.4268 sec/batch\n",
      "Epoch 5/20  Iteration 766/3560 Training loss: 1.7161 5.1548 sec/batch\n",
      "Epoch 5/20  Iteration 767/3560 Training loss: 1.7157 5.1403 sec/batch\n",
      "Epoch 5/20  Iteration 768/3560 Training loss: 1.7156 4.4656 sec/batch\n",
      "Epoch 5/20  Iteration 769/3560 Training loss: 1.7157 4.9322 sec/batch\n",
      "Epoch 5/20  Iteration 770/3560 Training loss: 1.7151 6.1606 sec/batch\n",
      "Epoch 5/20  Iteration 771/3560 Training loss: 1.7143 5.0819 sec/batch\n",
      "Epoch 5/20  Iteration 772/3560 Training loss: 1.7147 5.5281 sec/batch\n",
      "Epoch 5/20  Iteration 773/3560 Training loss: 1.7144 5.5733 sec/batch\n",
      "Epoch 5/20  Iteration 774/3560 Training loss: 1.7147 5.6101 sec/batch\n",
      "Epoch 5/20  Iteration 775/3560 Training loss: 1.7147 4.5033 sec/batch\n",
      "Epoch 5/20  Iteration 776/3560 Training loss: 1.7147 4.9110 sec/batch\n",
      "Epoch 5/20  Iteration 777/3560 Training loss: 1.7146 5.4334 sec/batch\n",
      "Epoch 5/20  Iteration 778/3560 Training loss: 1.7147 5.0839 sec/batch\n",
      "Epoch 5/20  Iteration 779/3560 Training loss: 1.7147 6.1848 sec/batch\n",
      "Epoch 5/20  Iteration 780/3560 Training loss: 1.7141 6.4471 sec/batch\n",
      "Epoch 5/20  Iteration 781/3560 Training loss: 1.7140 5.5800 sec/batch\n",
      "Epoch 5/20  Iteration 782/3560 Training loss: 1.7137 5.5484 sec/batch\n",
      "Epoch 5/20  Iteration 783/3560 Training loss: 1.7140 4.8745 sec/batch\n",
      "Epoch 5/20  Iteration 784/3560 Training loss: 1.7141 4.7048 sec/batch\n",
      "Epoch 5/20  Iteration 785/3560 Training loss: 1.7144 6.1205 sec/batch\n",
      "Epoch 5/20  Iteration 786/3560 Training loss: 1.7138 5.7492 sec/batch\n",
      "Epoch 5/20  Iteration 787/3560 Training loss: 1.7135 5.4157 sec/batch\n",
      "Epoch 5/20  Iteration 788/3560 Training loss: 1.7135 4.9909 sec/batch\n",
      "Epoch 5/20  Iteration 789/3560 Training loss: 1.7131 5.2430 sec/batch\n",
      "Epoch 5/20  Iteration 790/3560 Training loss: 1.7128 4.7908 sec/batch\n",
      "Epoch 5/20  Iteration 791/3560 Training loss: 1.7121 4.5823 sec/batch\n",
      "Epoch 5/20  Iteration 792/3560 Training loss: 1.7117 5.5817 sec/batch\n",
      "Epoch 5/20  Iteration 793/3560 Training loss: 1.7109 5.0625 sec/batch\n",
      "Epoch 5/20  Iteration 794/3560 Training loss: 1.7107 4.8448 sec/batch\n",
      "Epoch 5/20  Iteration 795/3560 Training loss: 1.7099 4.7669 sec/batch\n",
      "Epoch 5/20  Iteration 796/3560 Training loss: 1.7097 5.7418 sec/batch\n",
      "Epoch 5/20  Iteration 797/3560 Training loss: 1.7090 5.2435 sec/batch\n",
      "Epoch 5/20  Iteration 798/3560 Training loss: 1.7085 4.6259 sec/batch\n",
      "Epoch 5/20  Iteration 799/3560 Training loss: 1.7081 4.6356 sec/batch\n",
      "Epoch 5/20  Iteration 800/3560 Training loss: 1.7076 4.9560 sec/batch\n",
      "Validation loss: 1.56935 Saving checkpoint!\n",
      "Epoch 5/20  Iteration 801/3560 Training loss: 1.7077 4.4621 sec/batch\n",
      "Epoch 5/20  Iteration 802/3560 Training loss: 1.7076 4.3316 sec/batch\n",
      "Epoch 5/20  Iteration 803/3560 Training loss: 1.7071 4.5134 sec/batch\n",
      "Epoch 5/20  Iteration 804/3560 Training loss: 1.7067 4.2634 sec/batch\n",
      "Epoch 5/20  Iteration 805/3560 Training loss: 1.7060 4.6660 sec/batch\n",
      "Epoch 5/20  Iteration 806/3560 Training loss: 1.7055 5.7302 sec/batch\n",
      "Epoch 5/20  Iteration 807/3560 Training loss: 1.7049 5.0655 sec/batch\n",
      "Epoch 5/20  Iteration 808/3560 Training loss: 1.7046 4.2783 sec/batch\n",
      "Epoch 5/20  Iteration 809/3560 Training loss: 1.7042 4.6770 sec/batch\n",
      "Epoch 5/20  Iteration 810/3560 Training loss: 1.7036 4.2214 sec/batch\n",
      "Epoch 5/20  Iteration 811/3560 Training loss: 1.7030 4.9491 sec/batch\n",
      "Epoch 5/20  Iteration 812/3560 Training loss: 1.7023 5.5822 sec/batch\n",
      "Epoch 5/20  Iteration 813/3560 Training loss: 1.7019 4.7177 sec/batch\n",
      "Epoch 5/20  Iteration 814/3560 Training loss: 1.7016 5.0872 sec/batch\n",
      "Epoch 5/20  Iteration 815/3560 Training loss: 1.7011 4.7564 sec/batch\n",
      "Epoch 5/20  Iteration 816/3560 Training loss: 1.7007 4.9352 sec/batch\n",
      "Epoch 5/20  Iteration 817/3560 Training loss: 1.7002 4.3685 sec/batch\n",
      "Epoch 5/20  Iteration 818/3560 Training loss: 1.6998 4.4917 sec/batch\n",
      "Epoch 5/20  Iteration 819/3560 Training loss: 1.6994 5.7017 sec/batch\n",
      "Epoch 5/20  Iteration 820/3560 Training loss: 1.6991 4.9699 sec/batch\n",
      "Epoch 5/20  Iteration 821/3560 Training loss: 1.6989 4.9354 sec/batch\n",
      "Epoch 5/20  Iteration 822/3560 Training loss: 1.6986 6.3440 sec/batch\n",
      "Epoch 5/20  Iteration 823/3560 Training loss: 1.6981 5.5868 sec/batch\n",
      "Epoch 5/20  Iteration 824/3560 Training loss: 1.6978 4.7555 sec/batch\n",
      "Epoch 5/20  Iteration 825/3560 Training loss: 1.6974 6.1902 sec/batch\n",
      "Epoch 5/20  Iteration 826/3560 Training loss: 1.6971 4.5706 sec/batch\n",
      "Epoch 5/20  Iteration 827/3560 Training loss: 1.6966 4.5553 sec/batch\n",
      "Epoch 5/20  Iteration 828/3560 Training loss: 1.6960 5.9684 sec/batch\n",
      "Epoch 5/20  Iteration 829/3560 Training loss: 1.6957 5.5799 sec/batch\n",
      "Epoch 5/20  Iteration 830/3560 Training loss: 1.6954 5.2198 sec/batch\n",
      "Epoch 5/20  Iteration 831/3560 Training loss: 1.6951 5.0352 sec/batch\n",
      "Epoch 5/20  Iteration 832/3560 Training loss: 1.6948 5.0394 sec/batch\n",
      "Epoch 5/20  Iteration 833/3560 Training loss: 1.6944 5.1653 sec/batch\n",
      "Epoch 5/20  Iteration 834/3560 Training loss: 1.6938 4.8561 sec/batch\n",
      "Epoch 5/20  Iteration 835/3560 Training loss: 1.6933 5.3889 sec/batch\n",
      "Epoch 5/20  Iteration 836/3560 Training loss: 1.6931 6.5020 sec/batch\n",
      "Epoch 5/20  Iteration 837/3560 Training loss: 1.6928 5.3383 sec/batch\n",
      "Epoch 5/20  Iteration 838/3560 Training loss: 1.6922 5.4280 sec/batch\n",
      "Epoch 5/20  Iteration 839/3560 Training loss: 1.6920 6.3869 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a6e3d04c27a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                                                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                                                 model.optimizer],\n\u001b[0;32m---> 37\u001b[0;31m                                                feed_dict=feed)\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aleksandar/anaconda2/envs/tflearn/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aleksandar/anaconda2/envs/tflearn/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aleksandar/anaconda2/envs/tflearn/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/aleksandar/anaconda2/envs/tflearn/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aleksandar/anaconda2/envs/tflearn/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab),\n",
    "                 batch_size=batch_size,\n",
    "                 num_steps=num_steps,\n",
    "                 learning_rate=learning_rate,\n",
    "                 lstm_size=lstm_size,\n",
    "                 num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line velow to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/____.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches*epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x,y) in enumerate(get_batch([train_x,train_y],\n",
    "                                           num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, \n",
    "                                                model.final_state,\n",
    "                                                model.optimizer],\n",
    "                                               feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i800_l512_v1.569.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512_v2.390.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512_v1.959.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i600_l512_v1.721.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i800_l512_v1.569.ckpt\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1,1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state],\n",
    "                                       feed_dict=feed)\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state],\n",
    "                                       feed_dict=feed)\n",
    "            \n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "            \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farty that wher the\n",
      "prencation at his\n",
      "break, which.\n",
      "\n",
      "\"Why don't the ceation, was no his bray heard and where'r yon,. \"Well, the sement wonder. I was not to her to her,\" And as she was that her\n",
      "shand, but he was a crulled him the peesion of has\n",
      "face, she was a ther work to the coust he had been with strilk wat, and she sead the chance, a der sat one on the stortes.\n",
      "\n",
      "She was natelly stiliged the morest of siming howe, and saling to a life. And her his say he went of her when the mary, so to serme the chalfes, and hard\n",
      "been how to though shim that that he was haver her fell the dorness was somped in shich and some whith wish had the samested\n",
      "and her hear, with a south astint at him would no doot. She was still, and\n",
      "had net to her him..\n",
      "\n",
      "All as the what still while as he did not the means was how his sand and see the down of the parest a danct out all of her\n",
      "angare and a mare sat that that hear the shall were samped and the chied the\n",
      "changiress the sate whore the sture and to the stope, to site to still the\n",
      "streating the sore.\n",
      "\n",
      "The dinning a wert of hissest time to say and homent as he would never\n",
      "was no\n",
      "hinge with the mecterse and\n",
      "stormed on his how the strates and how this\n",
      "south on to\n",
      "hush that the warts wat she would not side his ateratain,. At her hears and his\n",
      "werl of the dowing\n",
      "and that saw as\n",
      "this and her and the secter and a certice, but they seed only the meares out at\n",
      "it with the\n",
      "samilie of the colding the\n",
      "to the marr and she had been treet he\n",
      "was she\n",
      "canted that this ware wert hand to tell at tome and with a charres, to toot and the seright on the satt wife\n",
      "his beet of tha from the milling husbeld then the wert of the sermorte, he had belone her, though he had she had not here and\n",
      "and to simply thas strong to\n",
      "all on the chathing with a some of the cander he had stopped, ho saw the\n",
      "shook as how heard her, which the censares and a mettle warked that his\n",
      "comperting\n",
      "her shiled.\n",
      "\n",
      "\"Ald, it! Thin in the more word ther sone though and\n",
      "her himself that head to all at the \n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"checkpoints/i800_l512_v1.569.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
